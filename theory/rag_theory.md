# Retrieval-Augmented Generation (RAG) – Theory & Practical Example

## 1. Introduction

RAG (Retrieval-Augmented Generation) is a **hybrid approach** that combines:

- **Retrieval**: Fetching relevant documents or information from a dataset or knowledge base.
- **Generation**: Using a language model (LLM) to generate coherent responses based on the retrieved context.

**Key Idea:** Instead of relying solely on a language model's memory, RAG fetches external information to **improve accuracy, context, and up-to-date responses**.

---

## 2. Why RAG?

- Standard LLMs generate answers only based on what they **already know**.
- RAG improves this by:
  1. **Searching a knowledge base** for relevant information.
  2. **Injecting retrieved info** into the prompt for the LLM.

**Example:**

- LLM alone: “Explain quantum computing” → generic answer.
- RAG: Retrieves recent research papers → generates **accurate and updated answers**.

---

## 3. Core Components

### 3.1 Retriever

- Fetches relevant documents from a knowledge base.
- Methods:
  - **Sparse retrieval**: TF-IDF, BM25.
  - **Dense retrieval**: Vector embeddings + similarity search.
- Output: Top-k relevant documents.

### 3.2 Generator

- Transformer-based LLM that generates responses.
- Input: Query + retrieved documents.
- Output: Context-aware, coherent answer.

### 3.3 Fusion Strategy

- How retrieved information is combined with generation:
  - **RAG-Sequence**: Model generates answer token by token considering all docs.
  - **RAG-Token**: Each token is generated by marginalizing over all docs.

---

## 4. Workflow

1. **Query Input**: User asks a question.
2. **Retrieve**: Top-k relevant documents are fetched.
3. **Context Construction**: Combine retrieved docs + query.
4. **Generate**: LLM generates answer using the combined context.
5. **Output**: Accurate, context-aware response.

---

## 5. Advantages

- Handles **large knowledge bases** efficiently.
- Improves **factual accuracy** compared to vanilla LLMs.
- Can access **up-to-date information**.
- Flexible: works with different retrievers, generators, and datasets.

---

## 6. Limitations

- Dependent on quality of retrieved documents.
- Retrieval latency may **slow down responses**.
- Complexity: managing embeddings, indexes, and model.
- Harder to fine-tune than a standard LLM.

---

## 7. Example Use Case

**Scenario:** Medical chatbot answering drug side effects.

1. User: “What are the side effects of Drug X?”
2. Retriever fetches relevant medical documents.
3. Generator synthesizes the answer using retrieved context.
4. Output: Accurate, context-aware medical info.

---

## 8. Key Terms

- **Retriever** → Finds relevant documents.
- **Generator** → Produces the final response.
- **Top-k documents** → Most relevant retrieved documents.
- **Vector embedding** → Numerical representation of text for similarity search.
- **RAG-Sequence / RAG-Token** → Methods to fuse retrieved info into generation.

---

## 9. Practical Python Example

Here’s a **mini RAG example using HuggingFace Transformers**:

```python
from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration

# Load tokenizer, retriever, and RAG model
tokenizer = RagTokenizer.from_pretrained("facebook/rag-token-nq")
retriever = RagRetriever.from_pretrained("facebook/rag-token-nq", index_name="exact", use_dummy_dataset=True)
model = RagSequenceForGeneration.from_pretrained("facebook/rag-token-nq", retriever=retriever)

# Define query
query = "What is Retrieval-Augmented Generation (RAG)?"
inputs = tokenizer(query, return_tensors="pt")

# Generate answer
generated_ids = model.generate(input_ids=inputs["input_ids"])
answer = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

print("Query:", query)
print("Answer:", answer[0])

---
Explanation of Practical Steps:

Tokenizer → Converts text into model-readable tokens.

Retriever → Fetches relevant documents (dummy dataset here for simplicity).

Model → Combines retrieved docs + query to generate the answer.

Output → Prints a coherent, factual answer.
```
