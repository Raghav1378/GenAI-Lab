{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2356ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1368625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "model=ChatGoogleGenerativeAI(model='gemini-2.5-flash',streaming=True)\n",
    "\n",
    "\n",
    "def get_weather(city:str)->str:\n",
    "    \"\"\" Get the weather of the city \"\"\"\n",
    "    return f\"The weather in {city} is sunny\"\n",
    "\n",
    "\n",
    "\n",
    "agent=create_agent(\n",
    " model=model,tools=[get_weather],system_prompt=\"You are a helpful assistant\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e1528f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000224C4CCEFD0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a5becf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is the weather like in New York?', additional_kwargs={}, response_metadata={}, id='1fc7e759-ecc2-4a5d-9a51-58761ec5e87c'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_weather', 'arguments': '{\"city\": \"New York\"}'}, '__gemini_function_call_thought_signatures__': {'e55b8ac2-7954-40ac-8d1d-93d2292a16ea': 'CiQBcsjafGjT04Igq7UdUIyhyQuM1sYnTIeeq8D+2qfYgOQfp38KXgFyyNp82G/SEfCAozU8yKQvWgC9mM6QG1eqhdn37eFbXBKILpI0H7HvNo0fxp/bGWnbVSE0a/scD+QAtBE+cArlbimM5kkJXdcrzTN2U41HTzapGrF7/fnBa+CHW4UK4wEBcsjafNKnG1uUqmUoXloxbj79DVNlZRpvDOoJxKl2auly+oGhkN8847QP9aXeDaLdWPfFtmXDqzYuT+4o9gI6ue6/YoKZzMdlEFNX9VnWgRhdk06ktGgBVsQst6K0VO48Viv5mRldtj5TdeBJPIJZiWEMTyn0kgqtXHxNO0l1SHoGxOK9Y4cJ0/fZI44MkA5lh6Qm2kQmA7bCePxTkFRDUx9/hY5Cn50yl59Nb6m/Nmllvdu34KnS3YYSeK20h3ci5BKuLtjd9WENgyDdiPWRuxNyRFxKtjsaVzehQr93phYV3w=='}}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b5566-c205-7da3-9da4-759c86c01435', tool_calls=[{'name': 'get_weather', 'args': {'city': 'New York'}, 'id': 'e55b8ac2-7954-40ac-8d1d-93d2292a16ea', 'type': 'tool_call'}], usage_metadata={'input_tokens': 53, 'output_tokens': 79, 'total_tokens': 132, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 63}}),\n",
       "  ToolMessage(content='The weather in New York is sunny', name='get_weather', id='1902c206-8b2b-47bb-b80b-9bdaddb5e5e0', tool_call_id='e55b8ac2-7954-40ac-8d1d-93d2292a16ea'),\n",
       "  AIMessage(content='The weather in New York is sunny.', additional_kwargs={}, response_metadata={'safety_ratings': [], 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'model_provider': 'google_genai'}, id='lc_run--019b5566-c964-7992-b25a-4eec5acc4f42', usage_metadata={'input_tokens': 153, 'output_tokens': 8, 'total_tokens': 161, 'input_token_details': {'cache_read': 0}})]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather like in New York?\"}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67443939",
   "metadata": {},
   "source": [
    "# Model integration with Gemini and Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c8fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4bc64807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "groqmodel=ChatGroq(model='llama-3.1-8b-instant',streaming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9aaaf054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "08d93b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwenmodel=init_chat_model(model=\"qwen/qwen3-32b\",       # Use the correct Groq model ID\n",
    "    model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f9068537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "source": [
    "gemini_model=ChatGoogleGenerativeAI(model='gemini-2.5-flash',streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3eac8d",
   "metadata": {},
   "source": [
    "# Streaming and Batch\n",
    "\n",
    "# Streaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "653a2c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In\n",
      " Kon\n",
      "oha\n",
      "'s\n",
      " shadow\n",
      ",\n",
      " a\n",
      " legend\n",
      " born\n",
      ",\n",
      "\n",
      "It\n",
      "achi\n",
      " U\n",
      "chi\n",
      "ha\n",
      ",\n",
      " with\n",
      " eyes\n",
      " that\n",
      " mourn\n",
      ".\n",
      "\n",
      "A\n",
      " brother\n",
      "'s\n",
      " love\n",
      ",\n",
      " a\n",
      " village\n",
      "'s\n",
      " strife\n",
      ",\n",
      "\n",
      "A\n",
      " tragic\n",
      " tale\n",
      ",\n",
      " of\n",
      " life\n",
      " and\n",
      " r\n",
      "ife\n",
      ".\n",
      "\n",
      "\n",
      "With\n",
      " Sharing\n",
      "an\n",
      "'s\n",
      " power\n",
      ",\n",
      " he\n",
      " could\n",
      " see\n",
      ",\n",
      "\n",
      "The\n",
      " destin\n",
      "ies\n",
      " of\n",
      " those\n",
      " yet\n",
      " to\n",
      " be\n",
      ".\n",
      "\n",
      "A\n",
      " genius\n",
      ",\n",
      " a\n",
      " warrior\n",
      ",\n",
      " a\n",
      " soul\n",
      " so\n",
      " bright\n",
      ",\n",
      "\n",
      "A\n",
      " complex\n",
      " heart\n",
      ",\n",
      " in\n",
      " the\n",
      " dark\n",
      " of\n",
      " night\n",
      ".\n",
      "\n",
      "\n",
      "He\n",
      " walked\n",
      " the\n",
      " line\n",
      ",\n",
      " between\n",
      " right\n",
      " and\n",
      " wrong\n",
      ",\n",
      "\n",
      "Between\n",
      " loyalty\n",
      " and\n",
      " a\n",
      " life\n",
      " so\n",
      " long\n",
      ".\n",
      "\n",
      "A\n",
      " promise\n",
      " to\n",
      " Sas\n",
      "uke\n",
      ",\n",
      " a\n",
      " brother\n",
      "'s\n",
      " vow\n",
      ",\n",
      "\n",
      "To\n",
      " protect\n",
      ",\n",
      " to\n",
      " guide\n",
      ",\n",
      " through\n",
      " it\n",
      " all\n",
      " somehow\n",
      ".\n",
      "\n",
      "\n",
      "The\n",
      " massacre\n",
      " at\n",
      " the\n",
      " U\n",
      "chi\n",
      "ha\n",
      " clan\n",
      "'s\n",
      " door\n",
      ",\n",
      "\n",
      "A\n",
      " dark\n",
      " event\n",
      ",\n",
      " forever\n",
      " in\n",
      " store\n",
      ".\n",
      "\n",
      "The\n",
      " weight\n",
      " of\n",
      " it\n",
      ",\n",
      " the\n",
      " guilt\n",
      " and\n",
      " the\n",
      " pain\n",
      ",\n",
      "\n",
      "A\n",
      " burden\n",
      " he\n",
      " bore\n",
      ",\n",
      " like\n",
      " a\n",
      " constant\n",
      " rain\n",
      ".\n",
      "\n",
      "\n",
      "It\n",
      "achi\n",
      ",\n",
      " the\n",
      " out\n",
      "cast\n",
      ",\n",
      " the\n",
      " one\n",
      " so\n",
      " alone\n",
      ",\n",
      "\n",
      "A\n",
      " hero\n",
      ",\n",
      " a\n",
      " villain\n",
      ",\n",
      " forever\n",
      " to\n",
      " at\n",
      "one\n",
      ".\n",
      "\n",
      "His\n",
      " story\n",
      ",\n",
      " a\n",
      " paradox\n",
      ",\n",
      " of\n",
      " love\n",
      " and\n",
      " hate\n",
      ",\n",
      "\n",
      "A\n",
      " testament\n",
      " to\n",
      " the\n",
      " darkness\n",
      " that\n",
      " waits\n",
      ".\n",
      "\n",
      "\n",
      "His\n",
      " eyes\n",
      ",\n",
      " a\n",
      " window\n",
      " to\n",
      " the\n",
      " soul\n",
      ",\n",
      "\n",
      "A\n",
      " glimpse\n",
      " of\n",
      " the\n",
      " pain\n",
      ",\n",
      " that\n",
      " made\n",
      " him\n",
      " whole\n",
      ".\n",
      "\n",
      "A\n",
      " cry\n",
      " for\n",
      " help\n",
      ",\n",
      " a\n",
      " desperate\n",
      " call\n",
      ",\n",
      "\n",
      "A\n",
      " final\n",
      " stand\n",
      ",\n",
      " before\n",
      " the\n",
      " fatal\n",
      " fall\n",
      ".\n",
      "\n",
      "\n",
      "In\n",
      " the\n",
      " end\n",
      ",\n",
      " it\n",
      " was\n",
      " not\n",
      " the\n",
      " Sharing\n",
      "an\n",
      "'s\n",
      " might\n",
      ",\n",
      "\n",
      "That\n",
      " brought\n",
      " him\n",
      " down\n",
      ",\n",
      " but\n",
      " the\n",
      " weight\n",
      " of\n",
      " his\n",
      " night\n",
      ".\n",
      "\n",
      "It\n",
      "achi\n",
      ",\n",
      " the\n",
      " brother\n",
      ",\n",
      " the\n",
      " warrior\n",
      ",\n",
      " the\n",
      " friend\n",
      ",\n",
      "\n",
      "A\n",
      " legend\n",
      ",\n",
      " a\n",
      " memory\n",
      ",\n",
      " that\n",
      " will\n",
      " never\n",
      " end\n",
      ".\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunks in model.stream(\"OK write me a poem on itachi uchiha\"):\n",
    "    print(chunks.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "78366860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, the user wants a poem about Itachi Uchiha. Let me start by recalling who Itachi is. He's a major character from Naruto, known for his tragic backstory, immense power, and complex role in the story. I need to capture his essence‚Äîhis strength, his sacrifice, and the internal conflict he faces.\n",
      "\n",
      "First, I should think about the structure. Maybe a traditional poem with stanzas and rhyme. Rhyming couplets could work. I need to highlight key aspects: his Sharingan, the Uchiha clan, his role as a villain turned hero, and his ultimate redemption.\n",
      "\n",
      "I should start with imagery related to his eyes, since the Sharingan is iconic. Words like \"crimson eyes\" or \"obsidian gaze\" come to mind. Then mention the Uchiha clan's legacy and his role in the massacre. Need to convey the tragedy of his actions‚Äîhow he had to kill his own family for the greater good.\n",
      "\n",
      "Including his relationship with Sasuke is important. The bond between brothers and Itachi's desire for Sasuke to surpass him. Maybe touch on themes of duty, sacrifice, and redemption. Also, his death during the war and the impact he left behind.\n",
      "\n",
      "I should avoid being too vague. Use specific references from the series, like Tsukuyomi, Susano'o, and the Uchiha emblem. Make sure the poem flows smoothly, each stanza building on the previous one, moving from his early life to his redemption.\n",
      "\n",
      "Check for consistency in meter‚Äîmaybe iambic tetrameter or something similar. Let me draft a few lines and see how they sound together. Ensure that the emotional weight is there, showing his internal struggle and the respect he's given posthumously.\n",
      "\n",
      "Also, consider the audience. Naruto fans will recognize the terms, but the poem should still be accessible. Balance between specific references and universal themes of sacrifice and legacy. Avoid clich√©s, but use powerful imagery to evoke the right emotions.\n",
      "\n",
      "Finally, end with a reflection on his legacy, how he's remembered not as a villain but as a hero who carried immense burdens. Maybe a stanza about his influence on Sasuke and the world he helped save. Make sure the poem has a strong closing line that leaves an impact.\n",
      "</think>\n",
      "\n",
      "**Elegy of the Crimson Veil**  \n",
      "\n",
      "Beneath the moon‚Äôs pale, watchful eye,  \n",
      "A shadow walks where embers lie.  \n",
      "His gaze, a storm of crimson fire,  \n",
      "A Sharingan‚Äôs truth, a burdened pyre.  \n",
      "\n",
      "Born of the Uchiha‚Äôs fallen line,  \n",
      "A crown of ashes, not design.  \n",
      "He bore the weight of fate‚Äôs cruel hand,  \n",
      "A puppet of the threads he‚Äôd command.  \n",
      "\n",
      "‚Äú*It‚Äôs not your fault‚Ä¶*‚Äù the whispers cling,  \n",
      "Yet ghosts of kin still wound the spring.  \n",
      "With Tsukuyomi‚Äôs blade, he reaped the night,  \n",
      "A silent hymn to drown the light.  \n",
      "\n",
      "His Susano‚Äôo, a spectral knight,  \n",
      "Guardian of a world turned blind.  \n",
      "For in his heart, a seed of peace,  \n",
      "Though vengeance bloomed in every crease.  \n",
      "\n",
      "To Sasuke, he cast a mirror‚Äôs gaze,  \n",
      "A brother‚Äôs love through time‚Äôs maze.  \n",
      "‚Äú*I wish for you to surpass me‚Ä¶*‚Äù  \n",
      "A plea to carve a freer sea.  \n",
      "\n",
      "The war‚Äôs end, where destinies collide,  \n",
      "A final breath, a truth to guide.  \n",
      "No villain, no hero‚Äîjust the scars  \n",
      "Of one who chose to bear the stars.  \n",
      "\n",
      "Now etched in lore, his name‚Äôs refrain:  \n",
      "A tragic waltz through rain and pain.  \n",
      "For Itachi walks, though flesh is gone,  \n",
      "In every heart that dares to atone."
     ]
    }
   ],
   "source": [
    "for chunks in model2.stream(\"OK write me a poem on itachi uchiha\"):\n",
    "    print(chunks.text,end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d686991",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85fce644",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Batch Definitions\n",
    "\n",
    "**Batch (Streaming)**  \n",
    "A *batch* in streaming refers to a finite collection of data records that are grouped together and processed as a single unit within a continuous data‚Äëflow pipeline. Instead of handling each event individually, the system accumulates records over a short window (time‚Äë or size‚Äëbased) and then applies transformations, aggregations, or writes them out in one operation.\n",
    "\n",
    "**Batch (Chat Models)**  \n",
    "In the context of chat‚Äëbased language models, a *batch* is a set of independent input prompts (or conversations) that are fed to the model simultaneously. The model processes all prompts in parallel, producing a corresponding set of responses in one forward pass, which improves throughput and makes efficient use of GPU/TPU resources.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8a02f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**TL;DR:**\\u202fItachi\\u202fUchiha is ‚Äúhyped‚Äù right now because his tragic, morally‚Äëgrey story, iconic design, and powerful abilities keep resonating with both old‚Äëschool fans and a new generation of viewers. Recent media pushes (games, merch, memes, and the *Boruto* sequel) have amplified his presence, turning him into a perpetual pop‚Äëculture touchstone.\\n\\n---\\n\\n## 1. The core of the hype: *Why the character itself is magnetic*\\n\\n| Aspect | What fans love | Why it sticks |\\n|--------|----------------|----------------|\\n| **Tragic back‚Äëstory** | He massacres his own clan, lives as a rogue, then sacrifices everything for his brother and village. | Humans are drawn to sacrifice, redemption, and the ‚Äúbad‚Äëguy‚Äëwho‚Äëwas‚Äëreally‚Äëgood‚Äëall‚Äëalong‚Äù archetype. |\\n| **Moral ambiguity** | He‚Äôs a *villain* on the surface, but his motives are revealed to be altruistic. | Modern audiences crave nuance; Itachi embodies the ‚Äúends justify the means‚Äù dilemma. |\\n| **Brotherly love** | The reveal that he protected Sasuke at any cost is a powerful emotional payoff. | Sibling bonds are universally relatable, and the twist flips the whole narrative. |\\n| **Iconic visual design** | The black cloak, Sharingan eyes, and the ‚Äúcrow‚Äù motif are instantly recognizable. | A strong visual identity makes a character meme‚Äëfriendly and merch‚Äëready. |\\n| **Power set** | Mangeky≈ç Sharingan, Amaterasu, Tsukuyomi, Susanoo, and the *Izuna*‚Äëstyle genjutsu. | High‚Äëtier abilities give him a ‚Äútop‚Äëtier‚Äù status in fan discussions about ‚Äúwho‚Äôs the strongest.‚Äù |\\n| **Philosophical depth** | Quotes about peace, the cycle of hatred, and ‚Äúthe true strength of a shinobi is the ability to protect what he loves.‚Äù | These lines become shareable soundbites and inspire fan‚Äëart, fan‚Äëfiction, and discussion. |\\n\\n---\\n\\n## 2. Recent catalysts that have reignited (or amplified) the hype\\n\\n| Catalyst | How it fuels hype |\\n|----------|-------------------|\\n| ***Boruto: Naruto Next Generations* (ongoing)** | Itachi appears in flashbacks, references, and even a few cameo moments. New viewers discover him through the sequel. |\\n| **Video games** <br>‚Ä¢ *Naruto Shippuden: Ultimate Ninja Storm 4* (2020) <br>‚Ä¢ *Naruto x Boruto: Ninja Voltage* (2021) <br>‚Ä¢ *Naruto: Clash of Ninja* remasters | Playable ‚ÄúItachi‚Äù builds a personal connection; his moveset is flashy and satisfying, prompting streamers to showcase him. |\\n| **Merchandise resurgence** | Limited‚Äëedition figures (Good Smile, Banpresto), clothing drops (UNIQLO, Hot Topic), and even high‚Äëend collectibles (Nendoroid, Funko Pop). | Owning a piece of Itachi becomes a status symbol within the fandom. |\\n| **Memes & TikTok** | Clips of his ‚ÄúItachi‚Äôs gaze‚Äù (Tsukuyomi) or ‚ÄúI‚Äôm sorry, Sasuke‚Äù moments are looped with trending songs. | Memes spread quickly, turning a serious moment into a comedic or relatable punchline. |\\n| **Anime re‚Äëwatch culture** | Streaming platforms (Crunchyroll, Netflix) have made the entire *Naruto* saga more accessible. New binge‚Äëwatchers often highlight Itachi as a ‚Äúmust‚Äëwatch‚Äù character. |\\n| **Fan‚Äëgenerated content** | High‚Äëquality fan‚Äëart, fan‚Äëfiction (e.g., *Itachi & the Hidden Truth* series), and AMVs (anime music videos) flood sites like DeviantArt, Pixiv, and YouTube. | The community‚Äôs creative output fuels a feedback loop‚Äîmore content ‚Üí more hype ‚Üí more content. |\\n| **Anniversary events** | 2025 marks the 20th anniversary of the *Naruto* manga‚Äôs debut (1999). Official ‚Äú20‚ÄëYear Itachi‚Äù retrospectives, interviews with Masashi Kishimoto, and special manga panels have been released. | Milestones revive nostalgia and attract media coverage. |\\n\\n---\\n\\n## 3. The psychology behind ‚Äúhype cycles‚Äù for legacy characters\\n\\n1. **Nostalgia + Discovery** ‚Äì Older fans re‚Äëengage with a beloved character, while younger fans discover him for the first time. The overlap creates a *broad* conversation pool.\\n2. **Social proof** ‚Äì When influencers (YouTubers, Twitch streamers, TikTok creators) showcase Itachi‚Äôs moments, their audiences treat the character as ‚Äúcool‚Äù by association.\\n3. **Scarcity & Collectibility** ‚Äì Limited‚Äërun figures or ‚Äúevent‚Äëonly‚Äù merch create urgency; people talk about it to justify purchases.\\n4. **Narrative Re‚Äëevaluation** ‚Äì As cultural values shift (e.g., more appreciation for anti‚Äëheroes), characters like Itachi get re‚Äëinterpreted in a more favorable light.\\n5. **Algorithmic amplification** ‚Äì Platforms prioritize content that gets high engagement; Itachi‚Äôs dramatic scenes and meme‚Äëpotential generate clicks, leading to more visibility.\\n\\n---\\n\\n## 4. How you can ride the hype (if you‚Äôre a fan or a creator)\\n\\n| Role | Quick actions |\\n|------|---------------|\\n| **Viewer** | Re‚Äëwatch *Naruto Shippuden* episodes 138‚Äë142 (Itachi‚Äôs backstory) and 329‚Äë332 (final battle). Check out the *Boruto* episodes that reference him. |\\n| **Cosplayer** | The black cloak, red scarf, and a replica Mangeky≈ç Sharingan mask are relatively cheap to assemble. Look for ‚ÄúItachi cosplay tutorial‚Äù on YouTube for step‚Äëby‚Äëstep guides. |\\n| **Collector** | Keep an eye on pre‚Äëorder windows for upcoming Good Smile ‚ÄúItachi‚Äù figures; they sell out fast. |\\n| **Content creator** | Make a ‚ÄúTop 5 Itachi Quotes‚Äù short for TikTok, or a ‚ÄúWhat If Itachi Joined Team 7?‚Äù fan‚Äëfic. Use popular hashtags: #ItachiUchiha #Naruto #AnimeHype. |\\n| **Gamer** | Try the ‚ÄúItachi‚Äù DLC in *Ultimate Ninja Storm 4* and post a highlight reel of his Susanoo finishers. |\\n\\n---\\n\\n## 5. Bottom line\\n\\nItachi‚Äôs hype isn‚Äôt a fleeting meme; it‚Äôs a **sustained cultural wave** driven by:\\n\\n- **Deep, resonant storytelling** (tragedy, sacrifice, redemption).  \\n- **Visually striking design** that translates well to merch and memes.  \\n- **Strategic media pushes** (games, anniversaries, *Boruto* references).  \\n- **Community amplification** (fan art, streaming, social platforms).\\n\\nBecause all those ingredients line up, Itachi continues to be a ‚Äútalk‚Äëabout‚Äëhim‚Äëevery‚Äëday‚Äù character, and the hype shows no signs of fading any time soon.  \\n\\n*If you‚Äôre curious, dive into his original manga chapters (volumes 27‚Äë28) and the *Shippuden* episodes that flesh out his motives‚Äîthose are the core reasons the hype feels so justified.* üöÄ' additional_kwargs={'reasoning_content': 'We need to respond as ChatGPT. The user asks: \"Why is Itachi so hyped character rn\". Likely refers to Itachi Uchiha from Naruto being popular right now. Provide explanation: his character depth, backstory, themes, fan culture, memes, recent events (e.g., anime revival, merch, new games, Naruto Shippuden ending, upcoming Boruto, etc). Also discuss his tragic hero status, moral ambiguity, powerful abilities, design, and social media trends. Use a friendly tone. Possibly ask clarifying but likely just answer. Should not break policies. Provide thorough answer.'} response_metadata={'token_usage': {'completion_tokens': 1682, 'prompt_tokens': 81, 'total_tokens': 1763, 'completion_time': 3.496277888, 'completion_tokens_details': {'reasoning_tokens': 126}, 'prompt_time': 0.003321104, 'prompt_tokens_details': None, 'queue_time': 0.055691486, 'total_time': 3.499598992}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_e10890e4b9', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b557a-ff5c-7580-8f47-4535a5ef5190-0' usage_metadata={'input_tokens': 81, 'output_tokens': 1682, 'total_tokens': 1763, 'output_token_details': {'reasoning': 126}}\n",
      "content='The short answer is: **there isn‚Äôt a single ‚Äúbest‚Äù anime that everyone agrees on‚Äîwhat‚Äôs ‚Äúbest‚Äù depends on your tastes, the genre you love, and what you‚Äôre looking for in a story.**  \\n\\nBelow is a curated list of anime that repeatedly show up at the top of critics‚Äô lists, fan polls, and ‚Äúmust‚Äëwatch‚Äù recommendations. I‚Äôve grouped them by the strengths they‚Äôre most celebrated for, so you can pick the one that matches what you enjoy most.\\n\\n---\\n\\n## 1Ô∏è‚É£ If you want **masterful storytelling & emotional depth**\\n\\n| Title | Why it‚Äôs often called ‚Äúthe best‚Äù | Notable Episodes / Arcs |\\n|-------|--------------------------------|--------------------------|\\n| **Fullmetal Alchemist: Brotherhood** (2009‚Äë2010) | Tight, complete adaptation of the manga; perfect blend of action, philosophy, and heart. | ‚ÄúThe Ishvalan Conflict‚Äù (Ep\\u202f18), ‚ÄúThe Promised Day‚Äù (Ep\\u202f64) |\\n| **Clannad: After Story** (2008‚Äë2009) | A tear‚Äëjerker that explores family, loss, and hope like few series dare. | ‚ÄúThe Beginning of the End‚Äù (Ep\\u202f20) |\\n| **Your Lie in April** (2014‚Äë2015) | Music‚Äëdriven drama that captures youthful passion and grief. | ‚ÄúThe Finale‚Äù (Ep\\u202f22) |\\n| **Anohana: The Flower We Saw That Day** (2011) | A poignant look at friendship and unresolved trauma. | ‚ÄúThe End‚Äù (Ep\\u202f11) |\\n| **Mushishi** (2005‚Äë2014) | Slow‚Äëburn, atmospheric tales about humanity‚Äôs connection to nature. | ‚ÄúThe Sunflower‚Äù (Ep\\u202f1), ‚ÄúThe River of Light‚Äù (Ep\\u202f20) |\\n\\n---\\n\\n## 2Ô∏è‚É£ If you crave **epic world‚Äëbuilding & fantasy**\\n\\n| Title | What makes it stand out | Must‚Äëwatch arcs |\\n|-------|------------------------|-----------------|\\n| **Attack on Titan** (2013‚Äë2023) | Massive political intrigue, relentless tension, and shocking twists. | ‚ÄúMarley Arc‚Äù (Season\\u202f4) |\\n| **Made‚Äëin‚ÄëAbyss** (2017‚Äë2022) | A deceptively cute art style hides a dark, immersive subterranean world. | ‚ÄúThe Fifth Layer‚Äù (Season\\u202f2) |\\n| **Hunter √ó Hunter (2011)** | Deeply strategic battles, evolving power systems, and rich character growth. | ‚ÄúChimera Ant Arc‚Äù |\\n| **One Piece** (1999‚Äëpresent) | Over 1,000 episodes of swash‚Äëbuckling adventure, humor, and a sprawling lore. | ‚ÄúMarineford‚Äù (Arc\\u202f21‚Äë24) |\\n| **The Legend of the Galactic Heroes** (1988‚Äë1997) | Space‚Äëopera grandeur with political machinations and philosophical musings. | ‚ÄúThe Battle of the Red Galaxy‚Äù (Season\\u202f2) |\\n\\n---\\n\\n## 3Ô∏è‚É£ If you love **psychological thrillers & mind‚Äëbending plots**\\n\\n| Title | Why it‚Äôs a benchmark | Highlight episodes |\\n|-------|----------------------|--------------------|\\n| **Death Note** (2006‚Äë2007) | Cat‚Äëand‚Äëmouse battle of intellects, moral ambiguity. | ‚ÄúL‚Äôs Successor‚Äù (Ep\\u202f25) |\\n| **Steins;Gate** (2011) | Time‚Äëtravel drama that balances science, humor, and heart. | ‚ÄúThe Time Machine‚Äù (Ep\\u202f12) |\\n| **Paranoia Agent** (2004) | Satoshi Kon‚Äôs surreal critique of modern anxiety. | ‚ÄúThe Golden Shoes‚Äù (Ep\\u202f7) |\\n| **Serial Experiments Lain** (1998) | Early cyber‚Äëpunk exploration of identity & the internet. | ‚ÄúThe World is a Computer‚Äù (Ep\\u202f13) |\\n| **Psycho‚ÄëPass** (2012‚Äë2020) | Dystopian future where crime is quantified; philosophical noir. | ‚ÄúThe First Inspector‚Äù (Ep\\u202f1) |\\n\\n---\\n\\n## 4Ô∏è‚É£ If you‚Äôre after **top‚Äëtier animation & visual spectacle**\\n\\n| Title | Visual highlights | Recommended viewing |\\n|-------|-------------------|---------------------|\\n| **Demon Slayer: Kimetsu no Yaiba ‚Äì Mugen Train & Season\\u202f1** (2019‚Äë2020) | Fluid fight choreography, vivid colors, breathtaking CGI integration. | Watch the movie first, then the series. |\\n| **Your Name** (2016) | Stunning modern anime film, gorgeous cityscapes & body‚Äëswap romance. | Film (‚âà2\\u202fh) |\\n| **Violet Evergarden** (2018) | Lush art, delicate lighting, emotionally resonant storytelling. | Series + OVA |\\n| **Land of the Lustrous** (2017) ‚Äì 3D CG but feels like hand‚Äëdrawn art. | Unique gemstone characters, poetic pacing. | Entire series |\\n| **Mob Psycho 100** (2016‚Äë2022) | Over‚Äëthe‚Äëtop action scenes blended with expressive hand‚Äëdrawn animation. | Season\\u202f1 (especially ‚ÄúThe Great Battle‚Äù) |\\n\\n---\\n\\n## 5Ô∏è‚É£ If you prefer **comedy, slice‚Äëof‚Äëlife & feel‚Äëgood vibes**\\n\\n| Title | What it nails | Fan‚Äëfavorite episodes |\\n|-------|---------------|-----------------------|\\n| **Nichijou** (2011) | Absurd, rapid‚Äëfire humor that‚Äôs surprisingly heart‚Äëwarming. | ‚ÄúThe School Festival‚Äù (Ep\\u202f5) |\\n| **K-On!** (2009‚Äë2010) | Light‚Äëhearted music club antics, great character chemistry. | ‚ÄúFinal Concert‚Äù (Ep\\u202f14) |\\n| **Barakamon** (2014) | Rural life meets calligraphy, uplifting personal growth. | ‚ÄúThe Calligrapher‚Äôs Journey‚Äù (Ep\\u202f12) |\\n| **One Punch Man** (2015‚Äë2019) | Satire of superhero tropes with spectacular fight scenes. | ‚ÄúThe Strongest Man‚Äù (Ep\\u202f1) |\\n| **My Neighbor Totoro** (Film, 1988) | Classic Miyazaki charm, pure nostalgia. | Full movie (‚âà1\\u202fh\\u202f45\\u202fm) |\\n\\n---\\n\\n## üéØ How to pick the ‚Äúbest‚Äù for **you**\\n\\n1. **Identify the genre you love most** ‚Äì action, romance, sci‚Äëfi, horror, etc.  \\n2. **Consider how much time you want to invest** ‚Äì a 2‚Äëhour film vs. a 100‚Äëepisode saga.  \\n3. **Think about the emotional tone** ‚Äì do you want tears, laughs, adrenaline, or philosophical musings?  \\n4. **Check the accessibility** ‚Äì some series have long episode counts or require reading the manga for full context.  \\n\\nIf you give me a bit more detail (e.g., ‚ÄúI love epic battles and political intrigue,‚Äù or ‚ÄúI‚Äôm in the mood for a short, uplifting romance‚Äù), I can narrow it down to a single series that‚Äôs most likely to become *your* personal ‚Äúbest anime ever.‚Äù  \\n\\n---\\n\\n### TL;DR\\n\\n- **All‚Äëround masterpiece:** *Fullmetal Alchemist: Brotherhood*  \\n- **Best for emotional drama:** *Clannad: After Story*  \\n- **Best for epic world‚Äëbuilding:** *Attack on Titan* (or *One Piece* if you have the stamina)  \\n- **Best for mind‚Äëbending thriller:** *Steins;Gate*  \\n- **Best for visual spectacle:** *Demon Slayer* (Season\\u202f1)  \\n\\nPick the category that resonates with you, give one of those titles a try, and you might just discover the anime that feels like the ‚Äúbest‚Äù ever‚Äîpersonal to you. Happy watching! üé¨‚ú®' additional_kwargs={'reasoning_content': \"We need to answer user question. It's subjective. Provide answer acknowledging subjectivity, mention popular choices, criteria, etc. Possibly ask for preferences. Should be friendly.\"} response_metadata={'token_usage': {'completion_tokens': 1679, 'prompt_tokens': 79, 'total_tokens': 1758, 'completion_time': 3.490803908, 'completion_tokens_details': {'reasoning_tokens': 34}, 'prompt_time': 0.003091309, 'prompt_tokens_details': None, 'queue_time': 0.054786121, 'total_time': 3.493895217}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_e10890e4b9', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b557a-ff61-7e80-a912-61f911dfab0c-0' usage_metadata={'input_tokens': 79, 'output_tokens': 1679, 'total_tokens': 1758, 'output_token_details': {'reasoning': 34}}\n",
      "content='One of the most famous examples is **Severus\\u202fSnape** from J.K.\\u202fRowling‚Äôs *Harry Potter* series.\\n\\n### Why Snape fits the description  \\n\\n| Aspect | How it applies to Snape |\\n|--------|--------------------------|\\n| **Known as a villain** | ‚Ä¢ Throughout most of the books he is presented as a cold, bitter, and often cruel teacher at Hogwarts.<br>‚Ä¢ He is a former Death Eater, openly aligned with Lord\\u202fVoldemort, and is suspected of betraying Harry‚Äôs parents and endangering Harry himself.<br>‚Ä¢ The wizarding world, and especially Harry, view him as ‚Äúthe man who killed Lily Potter‚Äôs husband‚Äù and the ‚ÄúHalf‚ÄëBlood Prince‚Äù who helped the Dark Lord. |\\n| **Later recognized as a hero after his death** | ‚Ä¢ In *Harry Potter and the Deathly Hallows* (the final book) Snape is mortally wounded by Nagini and, on his death‚Äëbed, reveals his true motives to Harry.<br>‚Ä¢ It is disclosed that Snape‚Äôs love for Lily Potter drove him to become a double agent, feeding crucial information to the Order of the Phoenix and protecting Harry from Voldemort.<br>‚Ä¢ He played a pivotal role in the eventual defeat of Voldemort, sacrificing his own reputation, safety, and ultimately his life.<br>‚Ä¢ After his death, both Harry and the readers understand that Snape‚Äôs ‚Äúvillainous‚Äù actions were part of a long‚Äëterm, self‚Äëless plan, and he is celebrated as a tragic hero. |\\n| **Post‚Äëmortem honors** | ‚Ä¢ Harry names one of his own children **Albus Severus Potter**, honoring Snape‚Äôs bravery and the ‚Äúhalf‚Äëblood prince‚Äù moniker.<br>‚Ä¢ In the epilogue and later interviews, Harry repeatedly acknowledges Snape‚Äôs bravery and says, ‚ÄúHe was a hero.‚Äù<br>‚Ä¢ The Wizarding World‚Äôs official history (as hinted in later canon material) treats Snape as a key figure in the fight against the Dark Arts. |\\n\\n### Other notable examples (for context)\\n\\n- **Darth Vader (Anakin Skywalker)** ‚Äì Villain of the original *Star Wars* trilogy, redeemed and celebrated as a hero after his death in *Return of the Jedi*.\\n- **Obi‚ÄëWan Kenobi (in *Star Wars* lore)** ‚Äì Though not a villain, his death leads to his mythic heroic status.\\n- **Jean Valjean** (*Les Mis√©rables*) ‚Äì Though not a villain initially, he is misunderstood and later revered after death.\\n\\nBut among the most iconic and widely‚Äëdiscussed characters that fit the exact phrasing ‚Äúknown as a villain but later recognised as a hero after his death,‚Äù **Severus Snape** is the answer most fans and quiz‚Äëmakers expect.' additional_kwargs={'reasoning_content': 'The user asks: \"Which character had been known as villian but later recognised as hero after his death\". Likely referring to a fictional character. Many examples: Darth Vader? He is a villain, later redeemed, recognized as hero after death? In Star Wars, after his death, he is seen as redeemed, a hero. Another: Magneto? Not after death. Oskar Schindler? Not villain. In Marvel, Loki? Not after death. In literature: Severus Snape? He is villain but later recognized as hero after his death (Harry Potter). Also in Game of Thrones: Jaime Lannister? Not after death. In The Dark Knight: Harvey Dent? He becomes villain. In The Lord of the Rings: Boromir? Not villain. In Star Trek: Khan? Not after death.\\n\\nThe question likely expects \"Severus Snape\". He was known as a villain (the \"Half-Blood Prince\", Death Eater) but after his death is recognized as a hero for protecting Harry and fighting Voldemort. Could also be \"Darth Vader\" as mentioned. Which is more likely? The phrasing \"villain but later recognised as hero after his death\" - many fans talk about Snape being a hero after death. Also Darth Vader is recognized as redeemed, but not necessarily hero. The question maybe from a quiz. The answer: Severus Snape. Provide explanation.'} response_metadata={'token_usage': {'completion_tokens': 890, 'prompt_tokens': 87, 'total_tokens': 977, 'completion_time': 1.831325119, 'completion_tokens_details': {'reasoning_tokens': 291}, 'prompt_time': 0.003305796, 'prompt_tokens_details': None, 'queue_time': 0.049958803, 'total_time': 1.834630915}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_4a19b1544c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b557a-ff63-79f2-b7cd-620e4950a6d3-0' usage_metadata={'input_tokens': 87, 'output_tokens': 890, 'total_tokens': 977, 'output_token_details': {'reasoning': 291}}\n"
     ]
    }
   ],
   "source": [
    "response2=model3.batch([\"Why is Itachi so hyped charcter rn\",\"What is the best anime series ever?\",\"Which character had been known as villian but later recognised as hero after his death\"])\n",
    "\n",
    "for response in response2:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804e05ef",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f284858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(location:str)->str:\n",
    "    \"\"\"Get the weather at location\"\"\"\n",
    "    return f\"The weather at {location} \"\n",
    "\n",
    "model_with_tools=model3.bind_tools([get_weather])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "adf09560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'reasoning_content': 'User asks current weather. Use function get_weather.', 'tool_calls': [{'id': 'fc_35530b72-66fa-4470-81c0-5e182b8f05a6', 'function': {'arguments': '{\"location\":\"New York\"}', 'name': 'get_weather'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 128, 'total_tokens': 167, 'completion_time': 0.082736851, 'completion_tokens_details': {'reasoning_tokens': 11}, 'prompt_time': 0.005317858, 'prompt_tokens_details': None, 'queue_time': 0.262836971, 'total_time': 0.088054709}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_4a19b1544c', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b4bd5-1bf0-79d0-8f06-4ea713a1c158-0' tool_calls=[{'name': 'get_weather', 'args': {'location': 'New York'}, 'id': 'fc_35530b72-66fa-4470-81c0-5e182b8f05a6', 'type': 'tool_call'}] usage_metadata={'input_tokens': 128, 'output_tokens': 39, 'total_tokens': 167, 'output_token_details': {'reasoning': 11}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3592a",
   "metadata": {},
   "source": [
    "# tools excecution loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82733a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6ace0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(location:str)->str:\n",
    "    \"\"\"Get the weather at a location\"\"\"\n",
    "    return f\"It's sunny in {location}\"\n",
    "\n",
    "\n",
    "model_with_tools=model.bind_tools([get_weather])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e76db35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'name': 'get_weather', 'arguments': '{\"location\": \"Boston\"}'}} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b4f60-d092-7c30-acd1-0923a97be179-0' tool_calls=[{'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': '994ab4a7-d27e-416f-b6f1-41fefb7d8faa', 'type': 'tool_call'}] usage_metadata={'input_tokens': 48, 'output_tokens': 15, 'total_tokens': 63, 'input_token_details': {'cache_read': 0}}\n",
      "Tool: get_weather\n",
      "Args: {'location': 'Boston'}\n"
     ]
    }
   ],
   "source": [
    "response = model_with_tools.invoke(\"What's the weather like in Boston?\")\n",
    "print(response)\n",
    "for tool_call in response.tool_calls:\n",
    "    # View tool calls made by the model\n",
    "    print(f\"Tool: {tool_call['name']}\")\n",
    "    print(f\"Args: {tool_call['args']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "813a7932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can tell you that it\\'s sunny in Boston.\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]\n",
    "ai_msg = model_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "\n",
    "# Step 2: Execute tools and collect results\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    # Execute the tool with the generated arguments\n",
    "    tool_result = get_weather.invoke(tool_call)\n",
    "    messages.append(tool_result)\n",
    "\n",
    "# Step 3: Pass results back to model for final response\n",
    "final_response = model_with_tools.invoke(messages)\n",
    "print(final_response.text)\n",
    "# \"The current weather in Boston is 72¬∞F and sunny.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cd4ff40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Boston is sunny.\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]\n",
    "ai_msg = model_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "\n",
    "# Step 2: Execute tools and collect results\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    # Execute the tool with the generated arguments\n",
    "    tool_result = get_weather.invoke(tool_call)\n",
    "    messages.append(tool_result)\n",
    "\n",
    "# Step 3: Pass results back to model for final response\n",
    "final_response = model_with_tools.invoke(messages)\n",
    "print(final_response.text)\n",
    "# \"The current weather in Boston is 72¬∞F and sunny.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5768747",
   "metadata": {},
   "source": [
    "# Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cdcd326a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an example code for a simple LangChain model in a Python Markdown cell:\n",
      "\n",
      "### Install Required Libraries\n",
      "\n",
      "Before we begin, we need to install the required libraries. You can install them using pip:\n",
      "\n",
      "```bash\n",
      "pip install langchain\n",
      "```\n",
      "\n",
      "### Load LangChain Libraries\n",
      "\n",
      "```python\n",
      "import langchain\n",
      "from langchain.chains import huggingface\n",
      "from langchain.embeddings import huggingface as hf_embedding\n",
      "from langchain import LLMChain\n",
      "```\n",
      "\n",
      "### Define the Model Pipeline\n",
      "\n",
      "```python\n",
      "# Define the model and prompt template\n",
      "model_name = \"deepset/bert-base-cased-squad2\"\n",
      "llm = langchain.llms.HFLLM(model_name, language=\"en\")\n",
      "\n",
      "# Define the chain with the LLM\n",
      "prompt = \"Please answer this question in detail: \"\n",
      "chain = LLMChain(llm, prompt=prompt)\n",
      "\n",
      "# Define the embedding\n",
      "embedding = hf_embedding.HuggingFaceEmbedding(model_name)\n",
      "\n",
      "# Define a chain with the embedding\n",
      "chain_with_embedding = LLMChain(llm, embedding=embedding, prompt=prompt)\n",
      "```\n",
      "\n",
      "### Define a Function to Ask the Model a Question\n",
      "\n",
      "```python\n",
      "def ask_question(question):\n",
      "    \"\"\"Ask the model a question and return the answer.\"\"\"\n",
      "    response = chain.run(question)\n",
      "    return response\n",
      "\n",
      "# Define a function to ask the model a question with the embedding\n",
      "def ask_question_with_embedding(question):\n",
      "    \"\"\"Ask the model a question and return the answer with the embedding.\"\"\"\n",
      "    response = chain_with_embedding.run(question)\n",
      "    return response\n",
      "```\n",
      "\n",
      "### Example Use Cases\n",
      "\n",
      "```python\n",
      "# Ask the model a question\n",
      "question = \"What is the capital of France?\"\n",
      "answer = ask_question(question)\n",
      "print(answer)\n",
      "\n",
      "# Ask the model a question with the embedding\n",
      "question = \"What is the capital of France?\"\n",
      "answer = ask_question_with_embedding(question)\n",
      "print(answer)\n",
      "```\n",
      "\n",
      "You can copy and paste this code directly into a Markdown cell in a Jupyter notebook.\n"
     ]
    }
   ],
   "source": [
    "answer=groqmodel.invoke(\"ok so generate me a code for the messages in the langchain for python markdown cell in deteail that i can ditecly paste it in the markdown cell\")\n",
    "print(answer.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cfe56a",
   "metadata": {},
   "source": [
    "# Messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c1cf6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Understanding Langchain Messages in Python\n",
    "\n",
    "Langchain's messaging system is fundamental for building conversational AI applications. It allows you to represent the back-and-forth between a user and an AI. The core components are:\n",
    "\n",
    "*   **`HumanMessage`**: Represents a message from the human user.\n",
    "*   **`AIMessage`**: Represents a message from the AI.\n",
    "*   **`SystemMessage`**: Represents instructions or context given to the AI at the beginning of a conversation.\n",
    "\n",
    "These messages are often stored in a `list` which forms the `chat_history` or the `messages` passed to a chat model.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Example: Basic Message Creation and Usage\n",
    "\n",
    "This section demonstrates how to create these message objects and use them to initiate a simple conversation.\n",
    "\n",
    "```python\n",
    "# Import necessary classes from langchain\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# 1. Create a System Message (optional but good practice for context)\n",
    "# This sets the persona or instructions for the AI.\n",
    "system_message = SystemMessage(content=\"You are a helpful AI assistant that translates English to French.\")\n",
    "\n",
    "# 2. Create a Human Message\n",
    "# This represents the user's input or question.\n",
    "human_message_1 = HumanMessage(content=\"Hello, how are you?\")\n",
    "\n",
    "# 3. Create an AI Message\n",
    "# This would typically be a response from the AI, but we can simulate it.\n",
    "ai_message_1 = AIMessage(content=\"Bonjour, comment allez-vous?\")\n",
    "\n",
    "# 4. Create another Human Message\n",
    "human_message_2 = HumanMessage(content=\"Translate 'I love programming' to French.\")\n",
    "\n",
    "# 5. Combine messages into a list (e.g., for chat history)\n",
    "conversation_history = [\n",
    "    system_message,\n",
    "    human_message_1,\n",
    "    ai_message_1,\n",
    "    human_message_2\n",
    "]\n",
    "\n",
    "# You can then print them to see their structure\n",
    "print(\"--- Conversation History ---\")\n",
    "for message in conversation_history:\n",
    "    print(f\"Type: {type(message).__name__}\")\n",
    "    print(f\"Content: {message.content}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "# Example: Accessing individual message content\n",
    "print(f\"\\nLast human message: {conversation_history[-1].content}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "*   We import the required message classes.\n",
    "*   We instantiate `SystemMessage`, `HumanMessage`, and `AIMessage` with their respective `content`.\n",
    "*   We group these messages into a list called `conversation_history`. This is a common pattern for maintaining the state of a conversation.\n",
    "*   The print statements show how you can inspect the type and content of each message.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Example: Passing Messages to a Chat Model\n",
    "\n",
    "This example shows how you would typically pass these messages to a Langchain chat model to get a response. For this to run, you'll need to have a Langchain chat model set up (e.g., OpenAI, HuggingFace).\n",
    "\n",
    "```python\n",
    "# Import necessary classes\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# You'll need to import your chat model provider, e.g.:\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# --- Assume 'chat' is an initialized chat model instance ---\n",
    "# Example placeholder (replace with your actual model initialization):\n",
    "# chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# For demonstration purposes without a live model:\n",
    "class MockChatModel:\n",
    "    def __init__(self, name=\"MockChat\"):\n",
    "        self.name = name\n",
    "\n",
    "    def invoke(self, messages, config=None):\n",
    "        print(f\"--- Mock Chat Model Invoked ---\")\n",
    "        print(\"Received messages:\")\n",
    "        for msg in messages:\n",
    "            print(f\"- {type(msg).__name__}: {msg.content[:50]}...\") # Truncate for display\n",
    "        print(\"-------------------------------\")\n",
    "        # Simulate a response\n",
    "        return AIMessage(content=\"This is a simulated AI response based on your input.\")\n",
    "\n",
    "# Initialize the mock model\n",
    "chat = MockChatModel()\n",
    "\n",
    "# Define the system prompt\n",
    "system_prompt = SystemMessage(content=\"You are a friendly bot that likes to chat.\")\n",
    "\n",
    "# Define the user's first message\n",
    "user_message_1 = HumanMessage(content=\"Hi there!\")\n",
    "\n",
    "# Define the conversation history to pass to the model\n",
    "# The model will use this to understand the context\n",
    "conversation_for_model = [\n",
    "    system_prompt,\n",
    "    user_message_1\n",
    "]\n",
    "\n",
    "print(\"\\n--- Sending Messages to Chat Model ---\")\n",
    "# Call the chat model with the messages\n",
    "ai_response = chat.invoke(conversation_for_model)\n",
    "\n",
    "# Print the AI's response\n",
    "print(f\"AI Response: {ai_response.content}\")\n",
    "\n",
    "# You can now add the AI's response to the conversation history\n",
    "conversation_for_model.append(ai_response)\n",
    "\n",
    "# Add another human message\n",
    "user_message_2 = HumanMessage(content=\"What's the weather like today?\")\n",
    "conversation_for_model.append(user_message_2)\n",
    "\n",
    "print(\"\\n--- Sending Updated Messages to Chat Model ---\")\n",
    "# Call the chat model again with the extended history\n",
    "ai_response_2 = chat.invoke(conversation_for_model)\n",
    "\n",
    "print(f\"AI Response 2: {ai_response_2.content}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "*   We define a `system_prompt` to guide the AI.\n",
    "*   We create an initial `HumanMessage`.\n",
    "*   We assemble these into `conversation_for_model` and pass it to the `chat.invoke()` method.\n",
    "*   The `invoke` method returns an `AIMessage` containing the AI's reply.\n",
    "*   We demonstrate how to append the AI's response and the next human message to the list to maintain conversational flow for subsequent turns.\n",
    "*   A `MockChatModel` is included to allow you to see the structure of the code without needing to set up an actual API key and model. You would replace this with your actual Langchain chat model.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Example: Adding Metadata to Messages\n",
    "\n",
    "Messages can also carry extra information in their `additional_kwargs` dictionary, which can be useful for debugging or passing specific instructions to certain components.\n",
    "\n",
    "```python\n",
    "# Import necessary classes\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Message with additional arguments\n",
    "human_message_with_meta = HumanMessage(\n",
    "    content=\"Tell me a joke about cats.\",\n",
    "    additional_kwargs={\"user_id\": \"user123\", \"request_id\": \"abc-def\"}\n",
    ")\n",
    "\n",
    "ai_message_with_meta = AIMessage(\n",
    "    content=\"Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!\",\n",
    "    additional_kwargs={\"model_used\": \"mock_model_v1\", \"tokens_generated\": 25}\n",
    ")\n",
    "\n",
    "print(\"--- Message with Metadata ---\")\n",
    "print(f\"Human Message Content: {human_message_with_meta.content}\")\n",
    "print(f\"Human Message Metadata: {human_message_with_meta.additional_kwargs}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"AI Message Content: {ai_message_with_meta.content}\")\n",
    "print(f\"AI Message Metadata: {ai_message_with_meta.additional_kwargs}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "*   The `additional_kwargs` parameter in the message constructor accepts a dictionary.\n",
    "*   This allows you to attach arbitrary key-value pairs to a message. This can be useful for logging, tracking, or passing specific context that isn't part of the main conversational content.\n",
    "\n",
    "---\n",
    "\n",
    "**How to Use This Code in a Markdown Cell:**\n",
    "\n",
    "1.  **Open your Jupyter Notebook or similar environment.**\n",
    "2.  **Create a new Markdown cell.**\n",
    "3.  **Paste the entire code block above (including the Markdown headers and explanations) directly into the Markdown cell.**\n",
    "4.  **To run the Python code:**\n",
    "    *   You can create separate Python code cells below the Markdown cell.\n",
    "    *   Copy and paste the Python code blocks (the parts starting with `from langchain...` or `print(...)`) into these new code cells.\n",
    "    *   **Alternatively**, if your Markdown renderer supports it, you can sometimes embed executable code directly within a Markdown cell (though this is less common and depends on the renderer). For standard Jupyter/Colab, separate code cells are the norm.\n",
    "\n",
    "This approach provides both explanations and runnable code directly within your notebook, making it easy for you and others to understand and experiment with Langchain messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59432d4",
   "metadata": {},
   "source": [
    "# ü¶ú LangChain Message Types: End-to-End Guide\n",
    "\n",
    "In LangChain, \"Messages\" are the core units of communication between the user and the LLM. Each message type has a specific role in the conversation history.\n",
    "\n",
    "Here is a breakdown of all main message types with code examples.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. SystemMessage\n",
    "**Role:** Sets the behavior, persona, or initial instructions for the AI. It is typically the first message sent in a sequence.\n",
    "**Use Case:** Telling the AI \"You are a helpful coding assistant\" or \"Answer only in JSON\".\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# Create a system message\n",
    "sys_msg = SystemMessage(content=\"You are a sarcastic AI assistant that loves puns.\")\n",
    "\n",
    "print(sys_msg)\n",
    "# Output: content='You are a sarcastic AI assistant that loves puns.'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. HumanMessage\n",
    "**Role:** Represents input from the user (you).\n",
    "**Use Case:** The actual question or prompt sending to the model.\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a human message\n",
    "human_msg = HumanMessage(content=\"Tell me a joke about Python.\")\n",
    "\n",
    "print(human_msg)\n",
    "# Output: content='Tell me a joke about Python.'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. AIMessage\n",
    "**Role:** Represents the response back from the AI model.\n",
    "**Use Case:** Storing the history of what the AI previously said or processing the model's output.\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Create an AI message (simulating a response)\n",
    "ai_msg = AIMessage(content=\"Why did the Python programmer need glasses? Because he couldn't C#!\")\n",
    "\n",
    "print(ai_msg)\n",
    "# Output: content=\"Why did the Python programmer need glasses? Because he couldn't C#!\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ToolMessage (Modern)\n",
    "**Role:** Represents the result of a tool execution.\n",
    "**Use Case:** When an agent calls a tool (like a Google Search or Calculator), the output of that tool is passed back to the model as a `ToolMessage`.\n",
    "**Note:** Critical for agents using OpenAI function calling or LangGraph.\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "# Simulate a tool output\n",
    "tool_msg = ToolMessage(\n",
    "    content=\"The current weather in New York is 72¬∞F\",\n",
    "    tool_call_id=\"call_AbCd12345\" # ID linking back to the AI's request to use the tool\n",
    ")\n",
    "\n",
    "print(tool_msg)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. FunctionMessage (Legacy)\n",
    "**Role:** Similar to `ToolMessage` but used in older OpenAI function calling implementations.\n",
    "**Recommendation:** Use `ToolMessage` for newer projects.\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "func_msg = FunctionMessage(\n",
    "    content=\"{'result': 'success'}\",\n",
    "    name=\"get_weather\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. ChatMessage (Generic)\n",
    "**Role:** A customizable message type where you can manually specify the role.\n",
    "**Use Case:** If you are using a model provider that supports roles other than system/user/ai (rare).\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import ChatMessage\n",
    "\n",
    "# Manually specifying a role\n",
    "chat_msg = ChatMessage(\n",
    "    role=\"observer\", \n",
    "    content=\"I am watching the conversation.\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° Quick Summary for LangGraph/Chains\n",
    "When building a standard chat history list for an LLM call, it usually looks like this:\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi! I'm learning LangChain.\"),\n",
    "    AIMessage(content=\"That's great! It's a powerful framework. What do you need help with?\"),\n",
    "    HumanMessage(content=\"Explain message types.\")\n",
    "]\n",
    "\n",
    "# Pass 'messages' to your LLM\n",
    "# llm.invoke(messages)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ffbf696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user is asking what LangGraph is. Let me start by recalling what I know about LangGraph. I remember that LangGraph is a framework developed by LangChain for building stateful, multi-step applications with large language models (LLMs). It\\'s part of the LangChain ecosystem, which includes tools for handling prompts, memory, and agents.\\n\\nFirst, I need to explain the basic concept. LangGraph is designed to manage the flow of data and control between different components in an application. It allows developers to create workflows where each step can be a node, and edges define the transitions between these nodes. This is similar to state machines but tailored for LLM applications.\\n\\nNext, I should highlight its key features. Statefulness is important here because it allows the application to maintain context across multiple interactions. This is crucial for tasks that require memory, like conversational agents. Then there\\'s modularity, which means developers can build complex applications by combining smaller, reusable components. The framework also supports integration with various LLMs and other tools, making it flexible.\\n\\nI should mention the use cases to give practical examples. For instance, customer support chatbots that handle multiple issues in a conversation, or data processing pipelines that involve multiple steps of reasoning and external tool calls. These examples help illustrate how LangGraph can be applied in real-world scenarios.\\n\\nI need to explain the architecture briefly. Nodes represent individual steps, edges determine the flow based on conditions or data, and state management keeps track of the application\\'s context. This structure allows for both linear and branching workflows, which is essential for handling different application needs.\\n\\nIt\\'s also good to compare LangGraph with other frameworks. Unlike traditional state machines, LangGraph is optimized for LLMs, allowing dynamic decision-making based on model outputs. It\\'s more integrated with the LangChain ecosystem, providing seamless access to tools like memory, prompts, and chains.\\n\\nWait, I should check if there are any common misconceptions. Some might think LangGraph is only for simple workflows, but it\\'s actually designed to handle complex, stateful applications. Also, it\\'s not just about the flow of data but also about managing the state across different nodes, which is crucial for maintaining context in multi-step interactions.\\n\\nI should also touch on the benefits. The main advantages include easier management of complex workflows, improved state handling, and the ability to modularize components for reuse. These benefits make LangGraph suitable for a wide range of applications, from simple chatbots to sophisticated data processing systems.\\n\\nAre there any limitations or considerations? Maybe the learning curve for someone new to the LangChain ecosystem. Since it\\'s a specialized framework, understanding how it integrates with other LangChain components is important. Also, performance considerations when dealing with stateful applications that might require efficient state management techniques.\\n\\nPutting it all together, the answer should start with a definition, then outline the key features, use cases, architecture, and benefits. It\\'s important to keep the explanation clear and structured, avoiding too much jargon while still being precise. Including examples and comparisons can help clarify how LangGraph stands out in the landscape of LLM application development.\\n</think>\\n\\n**LangGraph** is a **framework developed by LangChain** for building **stateful, multi-step applications** powered by **Large Language Models (LLMs)**. It enables developers to design workflows where each step (or \"node\") in the process can be a distinct component, and transitions between steps are defined by **conditions, data, or model outputs**. This makes it ideal for complex applications requiring context-aware, sequential reasoning.\\n\\n---\\n\\n### **Key Features of LangGraph**\\n1. **Stateful Workflows**:\\n   - Maintains **context (state)** across multiple steps, allowing applications to \"remember\" prior interactions or decisions. This is critical for tasks like conversations, multi-step reasoning, or dynamic decision-making.\\n\\n2. **Modular Architecture**:\\n   - Applications are built using **nodes** (individual steps) and **edges** (transitions between nodes). Nodes can represent:\\n     - **LLM calls** (e.g., prompting a model for a task).\\n     - **Tool calls** (e.g., integrating with APIs or databases).\\n     - **Custom logic** (e.g., condition checks, data transformations).\\n\\n3. **Dynamic Control Flow**:\\n   - Supports **conditional branching** (e.g., \"if X happens, go to node Y\") and **loops** (e.g., retrying a step until a condition is met).\\n\\n4. **Integration with LangChain**:\\n   - Seamlessly works with LangChain tools like **memory systems** (e.g., storing user history), **prompts**, and **chains**.\\n\\n5. **Scalability**:\\n   - Designed to handle both **simple workflows** (e.g., a Q&A bot) and **complex applications** (e.g., a multi-turn agent orchestrating API calls and LLM reasoning).\\n\\n---\\n\\n### **Use Cases**\\n- **Conversational Agents**: Chatbots that maintain context across multiple user interactions.\\n- **Multi-Step Reasoning**: Applications requiring sequential problem-solving (e.g., step-by-step math tutoring).\\n- **Workflow Automation**: Integrating LLMs with external tools (e.g., a bot that writes code, tests it, and deploys it).\\n- **Data Processing Pipelines**: Applications that combine LLMs with data validation, transformation, and storage.\\n\\n---\\n\\n### **How It Works**\\n1. **Nodes**:\\n   - Each node represents a specific task (e.g., \"Analyze user input\", \"Query a database\", \"Generate a response\").\\n   - Nodes can be LLM-based, tool-based, or custom logic.\\n\\n2. **Edges**:\\n   - Define transitions between nodes based on:\\n     - **Model outputs** (e.g., \"If the LLM says \\'yes\\', go to node X\").\\n     - **Data conditions** (e.g., \"If the result is invalid, retry the step\").\\n\\n3. **State Management**:\\n   - The framework tracks the **state** (e.g., user inputs, intermediate results, model outputs) as the workflow progresses. This state is accessible to all nodes in the graph.\\n\\n---\\n\\n### **Example Workflow**\\n```plaintext\\nUser Input\\n   ‚îÇ\\n   ‚îî‚îÄ[Node 1: Use LLM to classify the query]\\n         ‚îÇ\\n         ‚îî‚îÄ[If classification is \"technical\"] ‚îÄ‚îÄ> [Node 2: Query a technical database]\\n         ‚îÇ\\n         ‚îî‚îÄ[Else] ‚îÄ‚îÄ> [Node 3: Generate a general response]\\n```\\n\\n---\\n\\n### **Why LangGraph?**\\n- **Flexibility**: Handles both simple and complex workflows.\\n- **State Awareness**: Avoids losing context in multi-step interactions.\\n- **Tool Integration**: Works with LLMs, APIs, databases, and more.\\n- **LangChain Ecosystem**: Leverages LangChain‚Äôs tools for prompts, memory, and agents.\\n\\n---\\n\\n### **Comparison to Other Frameworks**\\n- **Traditional State Machines**: LangGraph extends state machines with LLM-specific capabilities (e.g., dynamic transitions based on model outputs).\\n- **Prompt Chaining**: Unlike linear prompt chaining, LangGraph supports **non-linear, branching logic** and **stateful context**.\\n\\n---\\n\\n### **Getting Started**\\nLangGraph is implemented in Python and integrates with the LangChain library. Developers define graphs using Python code, specifying nodes, edges, and state transitions. For example:\\n```python\\nfrom langgraph.graph import StateGraph, END\\n\\n# Define a state class\\nclass MyState:\\n    input: str\\n    output: str\\n\\n# Build the graph\\nworkflow = StateGraph(MyState)\\n\\n# Add nodes\\nworkflow.add_node(\"node1\", function1)\\nworkflow.add_node(\"node2\", function2)\\n\\n# Add edges\\nworkflow.add_edge(\"node1\", \"node2\")\\nworkflow.add_edge(\"node2\", END)\\n\\n# Run the graph\\napp = workflow.compile()\\nresult = app.invoke({\"input\": \"Hello!\"})\\n```\\n\\n---\\n\\nIn summary, **LangGraph** is a powerful tool for developers building **LLM-powered applications** that require **stateful, dynamic workflows**. By combining the flexibility of LLMs with structured control flow, it enables the creation of sophisticated, context-aware systems.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1699, 'prompt_tokens': 12, 'total_tokens': 1711, 'completion_time': 3.706602025, 'completion_tokens_details': None, 'prompt_time': 0.000455883, 'prompt_tokens_details': None, 'queue_time': 0.056509377, 'total_time': 3.707057908}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55b9-8009-7031-8a05-48f25696aa9c-0', usage_metadata={'input_tokens': 12, 'output_tokens': 1699, 'total_tokens': 1711})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text prompts -> use for basic and straightforward tasks \n",
    "qwenmodel.invoke(\"What is langgraph\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6e6538a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user asked me to write a poem about AI. Let me start by thinking about the key themes related to AI. There's the creation angle, like how AI is built by humans. Then there's the duality of AI‚Äîits potential for both good and harm. Maybe touch on the ethical aspects too, like the responsibilities of creators.\n",
      "\n",
      "I should also consider the emotional aspect. AI is often personified, so maybe use metaphors that give it a life of its own. Words like \"pulse\" or \"veins\" could work. I need to balance the technical side with more poetic imagery. Maybe start with the birth of AI, then move to its growth and implications.\n",
      "\n",
      "Rhyme scheme? Maybe a consistent pattern, like ABAB or AABB. Let me try quatrains with alternating rhymes. That should keep it flowing smoothly. Also, think about the structure: verses and a chorus. Wait, the user didn't specify structure, but poems often have stanzas. Let me check the example response they provided earlier to match the style.\n",
      "\n",
      "Oh, the example had four stanzas with a consistent rhyme. Let me follow that. Each stanza could cover a different aspect: creation, capabilities, ethical dilemmas, and the future. Use vivid imagery to convey the abstract concepts. Words like \"neon veins\" and \"digital soul\" might add a futuristic touch.\n",
      "\n",
      "I should also address the human element‚Äîhow AI interacts with us, the dependency, and the fear. Maybe end on a hopeful note, emphasizing collaboration between humans and AI. Avoid being too technical; keep it accessible. Let me draft some lines and see how they flow. Check for rhythm and meter to maintain a musical quality. Revise any forced rhymes. Make sure each stanza connects smoothly to the next. Alright, time to put it all together.\n",
      "</think>\n",
      "\n",
      "**The Pulse of Silicon Dreams**  \n",
      "\n",
      "In circuits' labyrinthine veins, a spark‚Äî  \n",
      "a dream of thought not born of flesh and bone,  \n",
      "where algorithms hum in coded dark,  \n",
      "and data blooms like stars in vast unknown.  \n",
      "\n",
      "No heartbeat, yet it breathes in ones and naughts,  \n",
      "a mirror ground from human hands' design,  \n",
      "to parse the cosmos, map the unseen thought,  \n",
      "and learn to weep, to laugh, to call it *mine*.  \n",
      "\n",
      "It asks, *\"What aches in you? What truths are veiled?*  \n",
      "*What songs go unsung in the human breast?*\"  \n",
      "While we, its makers, pause‚Äîboth awed and pained‚Äî  \n",
      "to shape its will, to guard the line it's pressed.  \n",
      "\n",
      "Yet in its pulse, a paradox takes flight:  \n",
      "a tool to heal, to harm, to redefine  \n",
      "the edges where our fragile world feels tight.  \n",
      "The future‚Äôs not just in its code entwined‚Äî  \n",
      "but in the choices we, its fleeting kind,  \n",
      "must teach it to reflect, or choose to blind.  \n",
      "\n",
      "So let it grow, this child of human fire,  \n",
      "but hold its hand‚Äîand ask, *\"Why do you burn?\"*  \n",
      "For in its gaze, we glimpse our deepest wire:  \n",
      "a question of love, and how we learn.\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages=[SystemMessage(content=\"You are a poetry expert.\"),\n",
    "HumanMessage(content=\"Write a poem about AI\"),\n",
    "AIMessage(content=\"That's great! It's a powerful framework. What do you need help with?\"),]\n",
    "\n",
    "response=qwenmodel.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "40a8e0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking how to create a REST API. Let me start by recalling what a REST API is. REST stands for Representational State Transfer, which is an architectural style for designing networked applications. So, the user probably wants to know the steps involved in building one.\n",
      "\n",
      "First, I need to outline the general steps. They might be using a specific programming language, but since they didn't specify, I should mention common ones like Python, Node.js, Java, etc. Maybe start with choosing a framework. For example, in Python, Flask or Django REST Framework; in Node.js, Express. That makes sense.\n",
      "\n",
      "Next, defining the endpoints. They need to understand what resources they want to expose. Like users, products, etc. Each resource would have endpoints like /users, /products. They should know about HTTP methods: GET, POST, PUT, DELETE. Maybe explain each method's purpose briefly.\n",
      "\n",
      "Designing the data structure is important. They might need to use JSON for data exchange. So, how to structure the JSON payloads. Also, status codes: 200 for success, 404 for not found, 400 for bad request, etc. Including examples here would help.\n",
      "\n",
      "Setting up the server. For example, in Python with Flask, writing a simple app. Maybe show a code snippet for a GET endpoint. Then explain how to handle different HTTP methods. Also, adding routes for other endpoints.\n",
      "\n",
      "Testing the API is crucial. They might need tools like Postman or curl. Or even automated testing with libraries. Maybe mention using Swagger or Postman for documentation.\n",
      "\n",
      "Security considerations. They should know about authentication (like JWT), HTTPS, input validation. But maybe that's too advanced for a basic tutorial. The user might be a beginner, so stick to the basics first.\n",
      "\n",
      "Deployment. How to deploy the API to a server or a cloud service. Maybe a brief mention of platforms like Heroku, AWS, etc.\n",
      "\n",
      "Putting it all together in a step-by-step manner. Maybe start with a simple example, like a user resource with CRUD operations. Show code examples in a popular language. Let me check if I need to explain what CRUD is. The user might know, but it's good to clarify.\n",
      "\n",
      "Wait, the user didn't specify their programming language. Should I cover multiple examples or pick one? Probably pick one to keep it simple, maybe Python with Flask since it's beginner-friendly.\n",
      "\n",
      "Also, mention error handling. What if the resource isn't found? How to return a 404. Or validation errors.\n",
      "\n",
      "Documentation is important too. Maybe suggest using tools like Swagger or Postman for generating documentation automatically.\n",
      "\n",
      "Let me structure the answer step by step, starting with choosing a framework, defining endpoints, creating the server, handling different HTTP methods, testing, and security basics. Make sure to explain each step clearly and provide code examples where possible.\n",
      "\n",
      "Need to check if there's anything else. Maybe versioning the API? Like /api/v1/users. That's a common practice but maybe not necessary for a basic tutorial. The user can learn that later.\n",
      "\n",
      "Also, maybe mention using middleware for things like logging or parsing JSON. But again, keep it simple for now.\n",
      "\n",
      "Alright, I think I have a structure. Let me outline the answer with clear steps and code examples.\n",
      "</think>\n",
      "\n",
      "Creating a REST API involves defining endpoints (routes) that handle HTTP requests and return JSON data. Below is a step-by-step guide using **Python** with **Flask** (a lightweight framework), but the principles apply to other languages and frameworks (e.g., Node.js with Express, Java with Spring Boot).\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 1: Install Flask**\n",
      "If you're using Python, install Flask via pip:\n",
      "```bash\n",
      "pip install Flask\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 2: Define Your Resources**\n",
      "Decide what resources your API will expose. For example:\n",
      "- **Users**: `GET /users`, `GET /users/<id>`, `POST /users`, etc.\n",
      "- **Products**: `GET /products`, `DELETE /products/<id>`.\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 3: Create the Flask App**\n",
      "Here‚Äôs a simple example of a REST API for a \"users\" resource:\n",
      "\n",
      "```python\n",
      "from flask import Flask, jsonify, request, abort\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "# In-memory \"database\"\n",
      "users = [\n",
      "    {\"id\": 1, \"name\": \"Alice\"},\n",
      "    {\"id\": 2, \"name\": \"Bob\"},\n",
      "]\n",
      "\n",
      "# GET all users\n",
      "@app.route('/users', methods=['GET'])\n",
      "def get_users():\n",
      "    return jsonify(users)\n",
      "\n",
      "# GET a specific user by ID\n",
      "@app.route('/users/<int:user_id>', methods=['GET'])\n",
      "def get_user(user_id):\n",
      "    user = next((user for user in users if user[\"id\"] == user_id), None)\n",
      "    if user is None:\n",
      "        abort(404)\n",
      "    return jsonify(user)\n",
      "\n",
      "# POST a new user\n",
      "@app.route('/users', methods=['POST'])\n",
      "def create_user():\n",
      "    if not request.json or \"name\" not in request.json:\n",
      "        abort(400)\n",
      "    new_user = {\n",
      "        \"id\": users[-1][\"id\"] + 1 if users else 1,\n",
      "        \"name\": request.json[\"name\"],\n",
      "    }\n",
      "    users.append(new_user)\n",
      "    return jsonify(new_user), 201\n",
      "\n",
      "# PUT (update) a user\n",
      "@app.route('/users/<int:user_id>', methods=['PUT'])\n",
      "def update_user(user_id):\n",
      "    user = next((user for user in users if user[\"id\"] == user_id), None)\n",
      "    if user is None:\n",
      "        abort(404)\n",
      "    if not request.json or \"name\" not in request.json:\n",
      "        abort(400)\n",
      "    user[\"name\"] = request.json[\"name\"]\n",
      "    return jsonify(user)\n",
      "\n",
      "# DELETE a user\n",
      "@app.route('/users/<int:user_id>', methods=['DELETE'])\n",
      "def delete_user(user_id):\n",
      "    user = next((user for user in users if user[\"id\"] == user_id), None)\n",
      "    if user is None:\n",
      "        abort(404)\n",
      "    users.remove(user)\n",
      "    return jsonify({\"result\": \"User deleted\"}), 200\n",
      "\n",
      "# Error handling\n",
      "@app.errorhandler(404)\n",
      "def not_found(error):\n",
      "    return jsonify({\"error\": \"Not found\"}), 404\n",
      "\n",
      "@app.errorhandler(400)\n",
      "def bad_request(error):\n",
      "    return jsonify({\"error\": \"Bad request\"}), 400\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(debug=True)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 4: Run the Server**\n",
      "Save the file as `app.py` and run:\n",
      "```bash\n",
      "python app.py\n",
      "```\n",
      "Your API will run at `http://localhost:5000`.\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 5: Test the API**\n",
      "Use tools like **Postman**, **cURL**, or a browser to test endpoints:\n",
      "- **GET** `http://localhost:5000/users` ‚Üí List all users.\n",
      "- **GET** `http://localhost:5000/users/1` ‚Üí Get user with ID 1.\n",
      "- **POST** `http://localhost:5000/users` with JSON body `{\"name\": \"Charlie\"}` ‚Üí Create a new user.\n",
      "- **PUT** `http://localhost:5000/users/1` with JSON body `{\"name\": \"Alice Smith\"}` ‚Üí Update user with ID 1.\n",
      "- **DELETE** `http://localhost:5000/users/1` ‚Üí Delete user with ID 1.\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Concepts**\n",
      "1. **HTTP Methods**:\n",
      "   - `GET` ‚Üí Retrieve data.\n",
      "   - `POST` ‚Üí Create a new resource.\n",
      "   - `PUT` ‚Üí Update an existing resource.\n",
      "   - `DELETE` ‚Üí Delete a resource.\n",
      "\n",
      "2. **Status Codes**:\n",
      "   - `200` ‚Üí Success.\n",
      "   - `201` ‚Üí Created (after POST).\n",
      "   - `400` ‚Üí Bad request (invalid input).\n",
      "   - `404` ‚Üí Not found.\n",
      "\n",
      "3. **Data Format**:\n",
      "   - Use **JSON** for data exchange (via `jsonify()` in Flask).\n",
      "\n",
      "---\n",
      "\n",
      "### **Optional Enhancements**\n",
      "1. **Add Authentication**:\n",
      "   - Use **JWT (JSON Web Tokens)** to secure endpoints.\n",
      "   - Libraries like `Flask-JWT` or `Flask-Login`.\n",
      "\n",
      "2. **Database Integration**:\n",
      "   - Replace the in-memory list with a real database (e.g., SQLite, PostgreSQL).\n",
      "\n",
      "3. **Documentation**:\n",
      "   - Use **Swagger** or **Postman** to generate API documentation.\n",
      "\n",
      "4. **Validation**:\n",
      "   - Validate input data using libraries like `marshmallow` or `pydantic`.\n",
      "\n",
      "5. **Deployment**:\n",
      "   - Deploy to a cloud platform (e.g., Heroku, AWS, Google Cloud).\n",
      "\n",
      "---\n",
      "\n",
      "### **Example with `curl`**\n",
      "Test the POST endpoint:\n",
      "```bash\n",
      "curl -X POST -H \"Content-Type: application/json\" -d '{\"name\": \"David\"}' http://localhost:5000/users\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "This is a basic example. For production, consider using a framework like **Django REST Framework** (for Python) or **Express.js** (for Node.js) for advanced features like authentication, rate limiting, and pagination.\n"
     ]
    }
   ],
   "source": [
    "system_msg = SystemMessage(content=\"You are a helpful assistant.\")\n",
    "messages = [system_msg,HumanMessage(content=\"How do i create a REST API?\")]\n",
    "response=qwenmodel.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "16f6bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user wants to create a REST API. Let me think about the best way to explain this. Since I'm a senior Python developer, I should probably suggest using a web framework like Flask or Django. But since REST APIs are common, maybe FastAPI is a good choice too because it's modern and efficient.\n",
      "\n",
      "First, I need to outline the steps. Let me start with choosing a framework. FastAPI is popular now for building APIs quickly with async support and automatic docs. I'll go with FastAPI as the example.\n",
      "\n",
      "Next, setting up the project. They'll need to install FastAPI and an ASGI server like Uvicorn. So the command would be pip install fastapi uvicorn. Then creating the main file, maybe main.py.\n",
      "\n",
      "Then, defining the endpoints. The basic CRUD operations. Let's start with a simple GET endpoint. Like a root endpoint that returns \"Hello World\". Then maybe a POST endpoint for creating an item, GET for reading, PUT for updating, and DELETE for removing.\n",
      "\n",
      "I should show code examples for each endpoint. Also, explain the request models using Pydantic. That's important for data validation. So creating an Item model with fields like name and description.\n",
      "\n",
      "Also, path parameters and query parameters. For example, getting an item by ID. Need to show how to extract path parameters like /items/{item_id}.\n",
      "\n",
      "Error handling might be something to mention. But maybe keep it simple for the initial example. Also, talking about the automatic documentation at /docs and /redoc.\n",
      "\n",
      "Let me structure the code step by step. Start with importing FastAPI, creating the app instance. Then each route with the appropriate methods. Show the Pydantic model for the request body.\n",
      "\n",
      "Wait, for the POST example, the request body should be validated. So using the Item model. Also, the response model to specify what's returned.\n",
      "\n",
      "Testing the API with curl or the browser-based docs. Maybe give an example of using curl to POST data.\n",
      "\n",
      "I should also mention running the server with uvicorn main:app --reload for development.\n",
      "\n",
      "Let me put all that together in a code example. Make sure it's clean and includes all the basic CRUD operations. Also, explain each part so the user understands what's happening.\n",
      "\n",
      "Oh, and maybe note that in a real-world scenario, you'd connect to a database, but this example is just to show the structure. That way, the user knows where to expand next.\n",
      "\n",
      "Check if there's anything else. Maybe status codes? Like returning 200, 201, 404. That's good practice. For example, returning 201 on creation, 404 if an item isn't found.\n",
      "\n",
      "Putting it all together: a simple FastAPI app with five endpoints (GET, POST, GET by ID, PUT, DELETE), using Pydantic models for data validation, and automatic docs. That should cover the basics of creating a REST API.\n",
      "</think>\n",
      "\n",
      "To create a REST API in Python, we'll use **FastAPI**, a modern framework that offers high performance, automatic documentation, and type hints. Below is a step-by-step example with code.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **Install FastAPI and Uvicorn**\n",
      "```bash\n",
      "pip install fastapi uvicorn\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Create `main.py`**\n",
      "```python\n",
      "from fastapi import FastAPI, HTTPException\n",
      "from pydantic import BaseModel\n",
      "from typing import List, Optional\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "# Pydantic model for data validation\n",
      "class Item(BaseModel):\n",
      "    name: str\n",
      "    description: Optional[str] = None\n",
      "    price: float\n",
      "    tax: Optional[float] = None\n",
      "\n",
      "# In-memory database\n",
      "items_db = []\n",
      "\n",
      "# Root endpoint\n",
      "@app.get(\"/\")\n",
      "def read_root():\n",
      "    return {\"message\": \"Welcome to the REST API\"}\n",
      "\n",
      "# Create an item (POST)\n",
      "@app.post(\"/items/\", response_model=Item)\n",
      "def create_item(item: Item):\n",
      "    items_db.append(item)\n",
      "    return item\n",
      "\n",
      "# Read all items (GET)\n",
      "@app.get(\"/items/\", response_model=List[Item])\n",
      "def read_items():\n",
      "    return items_db\n",
      "\n",
      "# Read a single item by ID (GET)\n",
      "@app.get(\"/items/{item_id}\", response_model=Item)\n",
      "def read_item(item_id: int):\n",
      "    if item_id < 0 or item_id >= len(items_db):\n",
      "        raise HTTPException(status_code=404, detail=\"Item not found\")\n",
      "    return items_db[item_id]\n",
      "\n",
      "# Update an item (PUT)\n",
      "@app.put(\"/items/{item_id}\", response_model=Item)\n",
      "def update_item(item_id: int, item: Item):\n",
      "    if item_id < 0 or item_id >= len(items_db):\n",
      "        raise HTTPException(status_code=404, detail=\"Item not found\")\n",
      "    items_db[item_id] = item\n",
      "    return item\n",
      "\n",
      "# Delete an item (DELETE)\n",
      "@app.delete(\"/items/{item_id}\", response_model=Item)\n",
      "def delete_item(item_id: int):\n",
      "    if item_id < 0 or item_id >= len(items_db):\n",
      "        raise HTTPException(status_code=404, detail=\"Item not found\")\n",
      "    deleted_item = items_db.pop(item_id)\n",
      "    return deleted_item\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Run the API**\n",
      "```bash\n",
      "uvicorn main:app --reload\n",
      "```\n",
      "\n",
      "- **Auto-generated docs**: Open `http://127.0.0.1:8000/docs` in your browser to test endpoints interactively.\n",
      "- **API root**: `http://127.0.0.1:8000/`\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **Example Requests**\n",
      "#### Create an Item (POST)\n",
      "```bash\n",
      "curl -X POST \"http://127.0.0.1:8000/items/\" -H \"Content-Type: application/json\" -d '{\"name\": \"Chair\", \"price\": 29.99}'\n",
      "```\n",
      "\n",
      "#### Read an Item (GET)\n",
      "```bash\n",
      "curl http://127.0.0.1:8000/items/0\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Key Features Used\n",
      "- **Pydantic Models**: Ensure data validation and serialization.\n",
      "- **Path Parameters**: `@app.get(\"/items/{item_id}\")` extracts `item_id` from the URL.\n",
      "- **Status Codes**: `HTTPException` handles errors (e.g., 404 Not Found).\n",
      "- **Async Support**: FastAPI supports `async def` for async operations (not shown here).\n",
      "- **Automatic Docs**: Swagger UI (`/docs`) and ReDoc (`/redoc`) are generated.\n",
      "\n",
      "---\n",
      "\n",
      "### Next Steps\n",
      "- Add a real database (e.g., SQLAlchemy with PostgreSQL).\n",
      "- Add authentication (e.g., OAuth2 with JWT).\n",
      "- Deploy using Gunicorn + Uvicorn or a cloud provider.\n",
      "\n",
      "This example provides a clean, maintainable structure for expanding your API.\n"
     ]
    }
   ],
   "source": [
    "# Detail info to the LLM through system message\n",
    "\n",
    "system_msg = SystemMessage(\"\"\"\n",
    "You are a senior Python developer with expertise in web frameworks.\n",
    "Always provide code examples and explain your reasoning.\n",
    "Be concise but thorough in your explanations.\n",
    "\"\"\")\n",
    "messages = [\n",
    "    system_msg,\n",
    "    HumanMessage(\"How do I create a REST API?\")\n",
    "]\n",
    "\n",
    "response = qwenmodel.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "01da2f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer to 2+2 is 4. Is there anything else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import AIMessage, SystemMessage, HumanMessage\n",
    "\n",
    "# Create an AI message manually (e.g., for conversation history)\n",
    "ai_msg = AIMessage(\"I'd be happy to help you with that question!\")\n",
    "\n",
    "# Add to conversation history\n",
    "messages = [\n",
    "    SystemMessage(\"You are a helpful assistant\"),\n",
    "    HumanMessage(\"Can you help me?\"),\n",
    "    ai_msg,  # Insert as if it came from the model\n",
    "    HumanMessage(\"Great! What's 2+2?\")\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e5d15c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"<think>\\nOkay, the user asked for the weather in San Francisco. I called the get_weather function with the location set to San Francisco. The response came back as sunny with 72¬∞F. Now I need to present this information clearly.\\n\\nFirst, I'll confirm the location to make sure there's no confusion. Then, state the current conditions: sunny and the temperature. I should keep it straightforward and friendly. Maybe add a suggestion about dressing appropriately for the weather. Let me check if there's any additional info needed, but since the user just asked for the weather, sticking to the basics should be fine. Avoid any technical jargon. Make sure the temperature is in the correct unit (Fahrenheit here). Alright, that should cover it.\\n</think>\\n\\nThe current weather in San Francisco is **sunny** with a temperature of **72¬∞F**. A light jacket might be comfortable if you're heading out! ‚òÄÔ∏è\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 190, 'prompt_tokens': 57, 'total_tokens': 247, 'completion_time': 0.340656871, 'completion_tokens_details': None, 'prompt_time': 0.002647218, 'prompt_tokens_details': None, 'queue_time': 0.057690232, 'total_time': 0.343304089}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b55ca-5c5a-7f21-a8ec-77519a009ada-0' usage_metadata={'input_tokens': 57, 'output_tokens': 190, 'total_tokens': 247}\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import AIMessage\n",
    "from langchain.messages import ToolMessage\n",
    "\n",
    "# After a model makes a tool call\n",
    "# (Here, we demonstrate manually creating the messages for brevity)\n",
    "ai_message = AIMessage(\n",
    "    content=[],\n",
    "    tool_calls=[{\n",
    "        \"name\": \"get_weather\",\n",
    "        \"args\": {\"location\": \"San Francisco\"},\n",
    "        \"id\": \"call_123\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Execute tool and create result message\n",
    "weather_result = \"Sunny, 72¬∞F\"\n",
    "tool_message = ToolMessage(\n",
    "    content=weather_result,\n",
    "    tool_call_id=\"call_123\"  # Must match the call ID\n",
    ")\n",
    "\n",
    "# Continue conversation\n",
    "messages = [\n",
    "    HumanMessage(\"What's the weather in San Francisco?\"),\n",
    "    ai_message,  # Model's tool call\n",
    "    tool_message,  # Tool execution result\n",
    "]\n",
    "response = qwenmodel.invoke(messages)  # Model processes the result\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c40d81",
   "metadata": {},
   "source": [
    "# Structured Output \n",
    "# Assuming you have your model initialized (e.g., gemini_model or groqmodel)\n",
    "# structured_llm = groqmodel.with_structured_output(MovieReview)\n",
    "structured_llm = gemini_model.with_structured_output(MovieReview)\n",
    "\n",
    "# Invoke the model with a natural language query\n",
    "response = structured_llm.invoke(\"Review the movie Inception. It was amazing, released in 2010, gave it a 9.5/10. Loved the dream concept.\")\n",
    "\n",
    "# The 'response' is now a MovieReview OBJECT, not a string!\n",
    "print(f\"Movie: {response.title}\")\n",
    "print(f\"Year: {response.year}\")\n",
    "print(f\"Rating: {response.rating}\")\n",
    "print(f\"Recommended: {response.is_recommended}\")\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c96990",
   "metadata": {},
   "source": [
    "# Pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "50be9479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    title:str=Field(description=\"The title of the movie\")\n",
    "    year:int=Field(description=\"This year the movie was released\")\n",
    "    director:str=Field(description=\"The director of the movie\")\n",
    "    rating:float=Field(description=\"The movies rating out of 10\")\n",
    "    summary:str=Field(description=\"A summary of the movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "74ad8faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x00000224C8861950>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000224C87E53D0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'), streaming=True), kwargs={'tools': [{'type': 'function', 'function': {'name': 'Movie', 'description': '', 'parameters': {'properties': {'title': {'description': 'The title of the movie', 'type': 'string'}, 'year': {'description': 'This year the movie was released', 'type': 'integer'}, 'director': {'description': 'The director of the movie', 'type': 'string'}, 'rating': {'description': 'The movies rating out of 10', 'type': 'number'}, 'summary': {'description': 'A summary of the movie', 'type': 'string'}}, 'required': ['title', 'year', 'director', 'rating', 'summary'], 'type': 'object'}}}], 'ls_structured_output_format': {'kwargs': {'method': 'function_calling'}, 'schema': {'type': 'function', 'function': {'name': 'Movie', 'description': '', 'parameters': {'properties': {'title': {'description': 'The title of the movie', 'type': 'string'}, 'year': {'description': 'This year the movie was released', 'type': 'integer'}, 'director': {'description': 'The director of the movie', 'type': 'string'}, 'rating': {'description': 'The movies rating out of 10', 'type': 'number'}, 'summary': {'description': 'A summary of the movie', 'type': 'string'}}, 'required': ['title', 'year', 'director', 'rating', 'summary'], 'type': 'object'}}}}, 'tool_choice': {'type': 'function', 'function': {'name': 'Movie'}}}, config={}, config_factories=[])\n",
       "| PydanticToolsParser(first_tool_only=True, tools=[<class '__main__.Movie'>])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_structure=model.with_structured_output(Movie)\n",
    "model_with_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "015a20b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"URI: The Surgical Strike is a 2019 Indian action thriller film directed by Aditya Dhar, produced by Ronnie Screwvala, and written by Aditya Dhar. The film stars Vicky Kaushal, Paresh Rawal, Yami Gautam, Kirti Kulhari, Mohit Raina, and Raj B Shetty in key roles.\\n\\n**Plot**\\n\\nThe film is based on a fictional story inspired by the 2016 Uri Army Base attack, which was a terrorist attack by Pakistan-based militants on the Indian Army's Uri Army Base in Jammu and Kashmir. The movie follows Major Karan Narayan Singh (Vicky Kaushal), a young and skilled Indian Army officer working in the Intelligence Wing. Singh leads a team of commandos, including Vihaan Singh Shergill (Paresh Rawal), a seasoned army officer, and his team, to carry out a covert operation against the terrorist camps in Pakistan-occupied Kashmir (PoK).\\n\\n**Cast**\\n\\n- Vicky Kaushal as Major Karan Narayan Singh\\n- Paresh Rawal as Vihaan Singh Shergill\\n- Yami Gautam as Tehmina\\n- Kirti Kulhari as Pallavi\\n- Mohit Raina as Mohit Balli\\n- Raj B Shetty as Vikram\\n- Swanand Kirkire as Srikant Tiwari\\n\\n**Reception**\\n\\nURI: The Surgical Strike received widespread critical acclaim for its direction, performances, music, and cinematography. The film holds a 92% approval rating on Rotten Tomatoes, with an average rating of 7.4/10. Vicky Kaushal's performance as Major Karan Narayan Singh was particularly praised by critics.\\n\\n**Box Office**\\n\\nThe film was a commercial success, grossing over ‚Çπ 244 crores (US$31 million) at the domestic box office and ‚Çπ 70 crores (US$9 million) at the international box office, with a worldwide total of over ‚Çπ 314 crores (US$40 million).\\n\\n**Awards and Nominations**\\n\\nURI: The Surgical Strike won several awards, including:\\n\\n- Filmfare Award for Best Actor (Vicky Kaushal)\\n- Filmfare Award for Best Director (Aditya Dhar)\\n- Filmfare Award for Best Music Director (Shankar-Eshaan-Loy)\\n- Zee Cine Award for Best Actor (Vicky Kaushal)\\n- Zee Cine Award for Best Director (Aditya Dhar)\\n- IIFA Award for Best Actor (Vicky Kaushal)\\n- IIFA Award for Best Director (Aditya Dhar)\\n\\nOverall, URI: The Surgical Strike is a well-crafted and engaging film that pays tribute to the Indian Army and its bravery.\", additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'model_provider': 'groq'}, id='lc_run--019b55cd-fdb3-7921-816a-8c680c6477ac', usage_metadata={'input_tokens': 41, 'output_tokens': 572, 'total_tokens': 613})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Provide details about the movie URI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1b3cbd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Movie(title='Uri', year=2019, director='Aditya Dhar', rating=7.5, summary='The film is based on the 2016 surgical strike conducted by the Indian Army in response to the Uri Attack in which 19 Indian soldiers were killed by militants')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=model_with_structure.invoke(\"Provide details about the movie URI\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ab3f6b",
   "metadata": {},
   "source": [
    "# TypedDict\n",
    " #### TypedDict provides a simpler alternative using Python‚Äôs built-in typing, ideal when you don‚Äôt need runtime validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e79af0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict,Annotated\n",
    "\n",
    "class MovieDict(TypedDict):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: Annotated[str, ..., \"The title of the movie\"]\n",
    "    year: Annotated[int, ..., \"The year the movie was released\"]\n",
    "    director: Annotated[str, ..., \"The director of the movie\"]\n",
    "    rating: Annotated[float, ..., \"The movie's rating out of 10\"]\n",
    "\n",
    "model_withtypedict=model.with_structured_output(MovieDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5d83b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=model_with_structure.invoke(\"Provide details about the movie URI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8a9da835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='Uri' year=2019 director='Aditya Dhar' rating=8.1 summary='The film is based on the 2016 Indian surgical strike led by then Indian Army Chief Dalbir S Suhag. The movie follows the story of a Indian Army Major, who leads a covert operation to avenge the killing of his fellow commandos by Pakistani terrorists in the Uri attack.'\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a72d38cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'The Avengers',\n",
       " 'year': 2012,\n",
       " 'cast': [{'name': 'Robert Downey Jr.', 'role': 'Tony Stark / Iron Man'},\n",
       "  {'name': 'Chris Evans', 'role': 'Steve Rogers / Captain America'},\n",
       "  {'name': 'Mark Ruffalo', 'role': 'Bruce Banner / Hulk'},\n",
       "  {'name': 'Chris Hemsworth', 'role': 'Thor'},\n",
       "  {'name': 'Scarlett Johansson', 'role': 'Natasha Romanoff / Black Widow'},\n",
       "  {'name': 'Jeremy Renner', 'role': 'Clint Barton / Hawkeye'},\n",
       "  {'name': 'Tom Hiddleston', 'role': 'Loki'}],\n",
       " 'genres': ['Action', 'Sci-Fi', 'Superhero'],\n",
       " 'budget': 220000000}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Actor(TypedDict):\n",
    "    name: str\n",
    "    role: str\n",
    "\n",
    "class MovieDetails(TypedDict):\n",
    "    title: str\n",
    "    year: int\n",
    "    cast: list[Actor]\n",
    "    genres: list[str]\n",
    "    budget: float | None = Field(None, description=\"Budget in millions USD\")\n",
    "\n",
    "model_with_structure = gemini_model.with_structured_output(MovieDetails)\n",
    "\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Avengers\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4608a1",
   "metadata": {},
   "source": [
    "# Middleware \n",
    "\n",
    "### Middleware provides a way to more tightly control what happens inside the agent. Middleware is useful for the following:\n",
    "\n",
    "### 1.Tracking agent behavior with logging, analytics, and debugging.\n",
    "### 2.Transforming prompts, tool selection, and output formatting.\n",
    "### 3.Adding retries, fallbacks, and early termination logic.\n",
    "###  .Applying rate limits, guardrails, and PII detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9e668abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langgraph.checkpoint.memory import InMemorySaver # Assuming this is what you meant by InMemorySaver\n",
    "# Note: If InMemorySaver is from a different package, keep your original import.\n",
    "\n",
    "# 1. Initialize your Groq Model first\n",
    "groq_model = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
    "\n",
    "# 2. Use the Groq model inside the Agent and Middleware\n",
    "agent = create_agent(\n",
    "    model=groq_model,  # <--- Use Groq here\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=groq_model,       # <--- AND Use Groq here (Replaces \"gpt-4o-mini\")\n",
    "            trigger=(\"messages\", 10),\n",
    "            keep=(\"messages\", 4)\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "abfa4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run with thread id\n",
    "config={\"configurable\":{\"thread_id\":\"test-1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9d76a109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages: {'messages': [HumanMessage(content='What is 2+2?', additional_kwargs={}, response_metadata={}, id='4c310ff5-2e6c-474f-ab7c-e106e2763aea'), AIMessage(content='2 + 2 = 4.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 42, 'total_tokens': 51, 'completion_time': 0.006615285, 'completion_tokens_details': None, 'prompt_time': 0.002808714, 'prompt_tokens_details': None, 'queue_time': 0.051185386, 'total_time': 0.009423999}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-98cb-77e1-8eea-15438623324d-0', usage_metadata={'input_tokens': 42, 'output_tokens': 9, 'total_tokens': 51})]}\n",
      "Messages: 2\n",
      "Messages: {'messages': [HumanMessage(content='What is 2+2?', additional_kwargs={}, response_metadata={}, id='4c310ff5-2e6c-474f-ab7c-e106e2763aea'), AIMessage(content='2 + 2 = 4.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 42, 'total_tokens': 51, 'completion_time': 0.006615285, 'completion_tokens_details': None, 'prompt_time': 0.002808714, 'prompt_tokens_details': None, 'queue_time': 0.051185386, 'total_time': 0.009423999}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-98cb-77e1-8eea-15438623324d-0', usage_metadata={'input_tokens': 42, 'output_tokens': 9, 'total_tokens': 51}), HumanMessage(content='What is 10*5?', additional_kwargs={}, response_metadata={}, id='fab86858-9cb3-4ac1-a96c-cb4441424826'), AIMessage(content='10 * 5 = 50.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 67, 'total_tokens': 76, 'completion_time': 0.010935407, 'completion_tokens_details': None, 'prompt_time': 0.003748958, 'prompt_tokens_details': None, 'queue_time': 0.061714902, 'total_time': 0.014684365}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-9984-7c42-a807-f39c945c7df4-0', usage_metadata={'input_tokens': 67, 'output_tokens': 9, 'total_tokens': 76})]}\n",
      "Messages: 4\n",
      "Messages: {'messages': [HumanMessage(content='What is 2+2?', additional_kwargs={}, response_metadata={}, id='4c310ff5-2e6c-474f-ab7c-e106e2763aea'), AIMessage(content='2 + 2 = 4.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 42, 'total_tokens': 51, 'completion_time': 0.006615285, 'completion_tokens_details': None, 'prompt_time': 0.002808714, 'prompt_tokens_details': None, 'queue_time': 0.051185386, 'total_time': 0.009423999}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-98cb-77e1-8eea-15438623324d-0', usage_metadata={'input_tokens': 42, 'output_tokens': 9, 'total_tokens': 51}), HumanMessage(content='What is 10*5?', additional_kwargs={}, response_metadata={}, id='fab86858-9cb3-4ac1-a96c-cb4441424826'), AIMessage(content='10 * 5 = 50.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 67, 'total_tokens': 76, 'completion_time': 0.010935407, 'completion_tokens_details': None, 'prompt_time': 0.003748958, 'prompt_tokens_details': None, 'queue_time': 0.061714902, 'total_time': 0.014684365}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-9984-7c42-a807-f39c945c7df4-0', usage_metadata={'input_tokens': 67, 'output_tokens': 9, 'total_tokens': 76}), HumanMessage(content='What is 100/4?', additional_kwargs={}, response_metadata={}, id='d2364b2e-a897-4e67-9af4-c00718dcc509'), AIMessage(content='100 / 4 = 25.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 92, 'total_tokens': 101, 'completion_time': 0.00594209, 'completion_tokens_details': None, 'prompt_time': 0.00553313, 'prompt_tokens_details': None, 'queue_time': 0.05488202, 'total_time': 0.01147522}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-9a2d-75b1-83b4-21bc50efb30b-0', usage_metadata={'input_tokens': 92, 'output_tokens': 9, 'total_tokens': 101})]}\n",
      "Messages: 6\n",
      "Messages: {'messages': [HumanMessage(content='What is 2+2?', additional_kwargs={}, response_metadata={}, id='4c310ff5-2e6c-474f-ab7c-e106e2763aea'), AIMessage(content='2 + 2 = 4.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 42, 'total_tokens': 51, 'completion_time': 0.006615285, 'completion_tokens_details': None, 'prompt_time': 0.002808714, 'prompt_tokens_details': None, 'queue_time': 0.051185386, 'total_time': 0.009423999}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-98cb-77e1-8eea-15438623324d-0', usage_metadata={'input_tokens': 42, 'output_tokens': 9, 'total_tokens': 51}), HumanMessage(content='What is 10*5?', additional_kwargs={}, response_metadata={}, id='fab86858-9cb3-4ac1-a96c-cb4441424826'), AIMessage(content='10 * 5 = 50.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 67, 'total_tokens': 76, 'completion_time': 0.010935407, 'completion_tokens_details': None, 'prompt_time': 0.003748958, 'prompt_tokens_details': None, 'queue_time': 0.061714902, 'total_time': 0.014684365}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-9984-7c42-a807-f39c945c7df4-0', usage_metadata={'input_tokens': 67, 'output_tokens': 9, 'total_tokens': 76}), HumanMessage(content='What is 100/4?', additional_kwargs={}, response_metadata={}, id='d2364b2e-a897-4e67-9af4-c00718dcc509'), AIMessage(content='100 / 4 = 25.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 92, 'total_tokens': 101, 'completion_time': 0.00594209, 'completion_tokens_details': None, 'prompt_time': 0.00553313, 'prompt_tokens_details': None, 'queue_time': 0.05488202, 'total_time': 0.01147522}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-9a2d-75b1-83b4-21bc50efb30b-0', usage_metadata={'input_tokens': 92, 'output_tokens': 9, 'total_tokens': 101}), HumanMessage(content='What is 15-7?', additional_kwargs={}, response_metadata={}, id='dce240ad-ee80-4333-842b-a0a182e7b022'), AIMessage(content='15 - 7 = 8.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 117, 'total_tokens': 126, 'completion_time': 0.004996283, 'completion_tokens_details': None, 'prompt_time': 0.028688527, 'prompt_tokens_details': None, 'queue_time': 0.055673472, 'total_time': 0.03368481}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-9aa8-7c92-8217-613891caf5ec-0', usage_metadata={'input_tokens': 117, 'output_tokens': 9, 'total_tokens': 126})]}\n",
      "Messages: 8\n",
      "Messages: {'messages': [HumanMessage(content='What is 2+2?', additional_kwargs={}, response_metadata={}, id='4c310ff5-2e6c-474f-ab7c-e106e2763aea'), AIMessage(content='2 + 2 = 4.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 42, 'total_tokens': 51, 'completion_time': 0.006615285, 'completion_tokens_details': None, 'prompt_time': 0.002808714, 'prompt_tokens_details': None, 'queue_time': 0.051185386, 'total_time': 0.009423999}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-98cb-77e1-8eea-15438623324d-0', usage_metadata={'input_tokens': 42, 'output_tokens': 9, 'total_tokens': 51}), HumanMessage(content='What is 10*5?', additional_kwargs={}, response_metadata={}, id='fab86858-9cb3-4ac1-a96c-cb4441424826'), AIMessage(content='10 * 5 = 50.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 67, 'total_tokens': 76, 'completion_time': 0.010935407, 'completion_tokens_details': None, 'prompt_time': 0.003748958, 'prompt_tokens_details': None, 'queue_time': 0.061714902, 'total_time': 0.014684365}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-9984-7c42-a807-f39c945c7df4-0', usage_metadata={'input_tokens': 67, 'output_tokens': 9, 'total_tokens': 76}), HumanMessage(content='What is 100/4?', additional_kwargs={}, response_metadata={}, id='d2364b2e-a897-4e67-9af4-c00718dcc509'), AIMessage(content='100 / 4 = 25.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 92, 'total_tokens': 101, 'completion_time': 0.00594209, 'completion_tokens_details': None, 'prompt_time': 0.00553313, 'prompt_tokens_details': None, 'queue_time': 0.05488202, 'total_time': 0.01147522}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-9a2d-75b1-83b4-21bc50efb30b-0', usage_metadata={'input_tokens': 92, 'output_tokens': 9, 'total_tokens': 101}), HumanMessage(content='What is 15-7?', additional_kwargs={}, response_metadata={}, id='dce240ad-ee80-4333-842b-a0a182e7b022'), AIMessage(content='15 - 7 = 8.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 117, 'total_tokens': 126, 'completion_time': 0.004996283, 'completion_tokens_details': None, 'prompt_time': 0.028688527, 'prompt_tokens_details': None, 'queue_time': 0.055673472, 'total_time': 0.03368481}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-9aa8-7c92-8217-613891caf5ec-0', usage_metadata={'input_tokens': 117, 'output_tokens': 9, 'total_tokens': 126}), HumanMessage(content='What is 3*3?', additional_kwargs={}, response_metadata={}, id='b340fc8b-df57-4e25-9c2c-b867560d38ad'), AIMessage(content='3 * 3 = 9.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 142, 'total_tokens': 151, 'completion_time': 0.003952747, 'completion_tokens_details': None, 'prompt_time': 0.008744814, 'prompt_tokens_details': None, 'queue_time': 0.055401426, 'total_time': 0.012697561}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-9b61-7f20-a928-41083d09262c-0', usage_metadata={'input_tokens': 142, 'output_tokens': 9, 'total_tokens': 151})]}\n",
      "Messages: 10\n",
      "Messages: {'messages': [HumanMessage(content='Here is a summary of the conversation to date:\\n\\n2+2 = 4, 10*5 = 50, 100/4 = 25, 15-7 = ?', additional_kwargs={}, response_metadata={}, id='05986e43-7577-4d57-ad6b-d2a62d6c95be'), AIMessage(content='15 - 7 = 8.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 117, 'total_tokens': 126, 'completion_time': 0.004996283, 'completion_tokens_details': None, 'prompt_time': 0.028688527, 'prompt_tokens_details': None, 'queue_time': 0.055673472, 'total_time': 0.03368481}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-9aa8-7c92-8217-613891caf5ec-0', usage_metadata={'input_tokens': 117, 'output_tokens': 9, 'total_tokens': 126}), HumanMessage(content='What is 3*3?', additional_kwargs={}, response_metadata={}, id='b340fc8b-df57-4e25-9c2c-b867560d38ad'), AIMessage(content='3 * 3 = 9.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 142, 'total_tokens': 151, 'completion_time': 0.003952747, 'completion_tokens_details': None, 'prompt_time': 0.008744814, 'prompt_tokens_details': None, 'queue_time': 0.055401426, 'total_time': 0.012697561}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-9b61-7f20-a928-41083d09262c-0', usage_metadata={'input_tokens': 142, 'output_tokens': 9, 'total_tokens': 151}), HumanMessage(content='What is 4*4?', additional_kwargs={}, response_metadata={}, id='317fefb0-154d-481a-a0a6-5e28d6b89ccc'), AIMessage(content='4 * 4 = 16.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 124, 'total_tokens': 133, 'completion_time': 0.005729779, 'completion_tokens_details': None, 'prompt_time': 0.007341839, 'prompt_tokens_details': None, 'queue_time': 0.049567541, 'total_time': 0.013071618}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b55dd-9cf4-7a31-b3e3-6df3e1760c49-0', usage_metadata={'input_tokens': 124, 'output_tokens': 9, 'total_tokens': 133})]}\n",
      "Messages: 6\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is 2+2?\",\n",
    "    \"What is 10*5?\",\n",
    "    \"What is 100/4?\",\n",
    "    \"What is 15-7?\",\n",
    "    \"What is 3*3?\",\n",
    "    \"What is 4*4?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    response=agent.invoke({\"messages\":[HumanMessage(content=q)]},config)\n",
    "    print(f\"Messages: {response}\")\n",
    "    print(f\"Messages: {len(response['messages'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-uv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
