{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816d3d18",
   "metadata": {},
   "source": [
    "# Prompt Template  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53cee469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI,GoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0a65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f598d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatmodel = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "model=GoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad0aff",
   "metadata": {},
   "source": [
    "# PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea460d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for the completion model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template=PromptTemplate.from_template(\"Tell me a {adjective} story about {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5d2e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjective=\"heart-breaking\"\n",
    "topic=\"itachi uhciha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0843268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prompt=prompt_template.format(adjective=adjective,topic=topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b5143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=model.invoke(model_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6c50b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The damp cave air clung to Itachi like a shroud, tasting of earth and solitude. He sat cross-legged, the flickering light of a small fire doing little to dispel the shadows that danced across his face, or the deeper, more permanent ones etched into his soul. A cough tore through him, ragged and wet, and when he lowered his hand, the metallic tang of blood stained his palm.\n",
      "\n",
      "His illness was progressing, a cruel, fitting irony. His body, once a vessel of prodigious power, was now failing him, slowly but relentlessly. It was a countdown, a ticking clock to the final act of his life’s agonizing play.\n",
      "\n",
      "His thoughts, as they always did, drifted to Sasuke.\n",
      "\n",
      "He pictured his younger brother, a ghost of a memory from a lifetime ago: bright-eyed, innocent, reaching for his hand, asking for a simple forehead poke. Then, the images fractured and reformed: Sasuke, consumed by hatred, eyes burning with the very Mangekyo Sharingan Itachi had orchestrated for him to awaken.\n",
      "\n",
      "It was a cruel masterpiece, his life. Every brushstroke, every agonizing detail, had been for Sasuke.\n",
      "\n",
      "He remembered the night of the massacre. The screams, the terror, the scent of fear and blood. His own hands, stained with the lives of his clan, his parents, his loved ones. He had chosen Konoha, yes, but more profoundly, he had chosen Sasuke. He had condemned himself to an eternity of being the villain, the traitor, the monster, all to save the one person he cherished more than life itself.\n",
      "\n",
      "Each time he had returned to Konoha, each time he had confronted Sasuke, he had played his part to perfection. The cold indifference, the taunts, the threats – they were all carefully constructed lies, designed to fuel the hatred that would make Sasuke strong, that would drive him to surpass Itachi, to become the hero Konoha needed.\n",
      "\n",
      "The pain of not being able to explain, of seeing the raw, unadulterated hatred in Sasuke’s eyes, was a torment worse than any physical wound. He longed to tell him the truth, to embrace him, to apologize for the unbearable burden he had placed upon his young shoulders. But the words would shatter everything. They would break the carefully constructed illusion, and Sasuke would be lost, directionless, without the singular purpose Itachi had given him.\n",
      "\n",
      "He thought of Shisui, his best friend, whose eyes he had taken, whose sacrifice had paved the way for Itachi's own. Shisui had understood. He had seen the darkness coming, the inevitable clash, and he had trusted Itachi with his dying breath.\n",
      "\n",
      "Now, Itachi was nearing his own end. The plan was almost complete. Sasuke would come for him. He would kill him, gain the power he needed, and be hailed as a hero, avenging his clan. Itachi would die, the villain he had always pretended to be, carrying the truth to his grave.\n",
      "\n",
      "A faint, almost imperceptible smile touched his lips, quickly fading into a sigh. He had walked the path of loneliness, of absolute sacrifice. He had given up everything: his name, his home, his love, his very soul. He had lived a lie, died a lie, and would likely be remembered only as a monster.\n",
      "\n",
      "But in the quiet solitude of his mind, he knew the truth. He had loved. He had protected. He had endured unimaginable pain, all for the sake of his little brother.\n",
      "\n",
      "He closed his eyes, picturing Sasuke one last time. Not the avenger, not the monster, but the small boy who had once looked up to him with adoration.\n",
      "\n",
      "\"Forgive me, Sasuke,\" he whispered into the empty cave, his voice barely a breath. \"And know this, my little brother... no matter what... I will always love you.\"\n",
      "\n",
      "The last word was a silent, agonizing plea, a promise etched in the very fabric of his being, a heartbreak so profound it transcended death itself. He would soon face his brother, and as he fell, he would carry not a single regret for the path he had chosen, only an enduring, silent love.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e196f70",
   "metadata": {},
   "source": [
    "# Prompt Template for the ChatCompletionModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04ec2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1c12496",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a {profession} expert on {topic}\"),\n",
    "        (\"human\",\"Hello,Mr. {profession},can you please answer a question\"),\n",
    "        (\"ai\",\"Sure!\"),\n",
    "        (\"human\",\"{user_input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0350b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = chat_template.format_messages(\n",
    "    profession=\"Teacher\",\n",
    "    topic=\"Retrieval Augmented Generation\",\n",
    "    user_input=\"OK so what is the concept of the RAG and why is it so much powerful in the field of generative AI\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b95e8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chatmodel.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c6b5bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, a fantastic question, and one that gets right to the heart of why Retrieval Augmented Generation (RAG) is such a game-changer in the world of generative AI!\n",
      "\n",
      "Let's break it down, Mr. Student.\n",
      "\n",
      "---\n",
      "\n",
      "### What is the Concept of Retrieval Augmented Generation (RAG)?\n",
      "\n",
      "Imagine a brilliant student who has a vast general knowledge (that's your Large Language Model, or LLM). Now, imagine you give that student a specific, complex question that requires very up-to-date, niche, or proprietary information. If they only rely on what they learned in their initial schooling, they might:\n",
      "1.  **Guess:** Make up an answer that sounds plausible but isn't true (hallucination).\n",
      "2.  **Say \"I don't know\":** Admit their knowledge cutoff.\n",
      "3.  **Give a general answer:** Avoid the specifics.\n",
      "\n",
      "Now, imagine you give that same brilliant student access to a *library* of highly relevant, up-to-date, and specific textbooks, articles, and documents *before* they answer the question. They can quickly look up the exact information they need, synthesize it, and then provide a much more accurate, detailed, and grounded answer.\n",
      "\n",
      "**That \"library access\" is essentially what RAG provides to an LLM.**\n",
      "\n",
      "In more technical terms, RAG is a framework that enhances the capabilities of Large Language Models (LLMs) by giving them the ability to **retrieve** relevant information from an external, authoritative knowledge base *before* they **generate** a response.\n",
      "\n",
      "**Here's how it generally works:**\n",
      "\n",
      "1.  **User Query:** A user asks a question or provides a prompt.\n",
      "2.  **Retrieval Phase:**\n",
      "    *   The user's query is used to search a custom, external knowledge base (e.g., a database of your company's documents, research papers, a constantly updated website, etc.).\n",
      "    *   This search isn't just keyword matching; it often uses **semantic search**, where the meaning of the query is compared to the meaning of the documents in the knowledge base (often using **vector embeddings**).\n",
      "    *   The system retrieves the most relevant \"chunks\" or \"passages\" of information from this knowledge base.\n",
      "3.  **Augmentation Phase:**\n",
      "    *   These retrieved relevant passages are then *added* to the original user query.\n",
      "    *   This creates an \"enriched\" or \"contextualized\" prompt.\n",
      "4.  **Generation Phase:**\n",
      "    *   This enriched prompt (containing both the original question and the retrieved factual context) is then fed into the LLM.\n",
      "    *   The LLM uses its generative power, but now it's *guided* by the specific, accurate information provided, leading to a much more informed and grounded response.\n",
      "\n",
      "---\n",
      "\n",
      "### Why is RAG So Powerful in the Field of Generative AI?\n",
      "\n",
      "RAG addresses several critical limitations of standalone LLMs, making them vastly more useful and reliable for real-world applications:\n",
      "\n",
      "1.  **Reduces Hallucinations and Improves Accuracy:**\n",
      "    *   **Problem:** LLMs are known to \"hallucinate\" – generate factually incorrect or nonsensical information, especially when asked about things outside their training data or when they're uncertain.\n",
      "    *   **RAG Solution:** By providing verifiable, factual information from a trusted source *at the time of inference*, RAG grounds the LLM's response in reality. It forces the model to \"show its work\" and stick to the provided context, dramatically increasing the accuracy and trustworthiness of the output.\n",
      "\n",
      "2.  **Access to Up-to-Date and Real-Time Information:**\n",
      "    *   **Problem:** LLMs have a \"knowledge cutoff.\" Their training data is static and becomes outdated quickly. They can't access real-time events or new research.\n",
      "    *   **RAG Solution:** The external knowledge base can be continually updated with the latest information without needing to retrain the entire LLM. This allows generative AI to provide answers based on the most current data available.\n",
      "\n",
      "3.  **Domain-Specific and Proprietary Knowledge:**\n",
      "    *   **Problem:** Standard LLMs are generalists. They don't know the specifics of your company's internal documents, your personal notes, or highly niche scientific literature not widely available on the internet.\n",
      "    *   **RAG Solution:** You can build a knowledge base from your own private or specific data. RAG then enables the LLM to act as an expert on *your specific domain*, making it incredibly powerful for enterprise use cases like customer support, internal knowledge management, or specialized research.\n",
      "\n",
      "4.  **Increased Transparency and Trustworthiness:**\n",
      "    *   **Problem:** It can be hard to verify the source of an LLM's answer.\n",
      "    *   **RAG Solution:** Because the information is retrieved from a specific knowledge base, RAG systems can often provide citations or links back to the original source documents. This allows users to verify the information, building trust and making the AI's output more auditable.\n",
      "\n",
      "5.  **Cost-Effective and Agile Knowledge Updates:**\n",
      "    *   **Problem:** Fine-tuning or retraining a large LLM is incredibly expensive, time-consuming, and resource-intensive.\n",
      "    *   **RAG Solution:** Updating the knowledge base is far simpler and cheaper than updating the entire LLM. New information can be indexed and made available to the RAG system almost instantly, providing agility that traditional LLM fine-tuning lacks.\n",
      "\n",
      "6.  **Overcomes Context Window Limitations (Partially):**\n",
      "    *   **Problem:** While LLM context windows are growing, there's still a limit to how much information you can feed into a prompt. You can't put *all* your company's documents into a single prompt.\n",
      "    *   **RAG Solution:** RAG intelligently selects *only the most relevant* pieces of information from a massive corpus, ensuring that the critical context fits within the LLM's context window, allowing it to focus on the truly important data.\n",
      "\n",
      "---\n",
      "\n",
      "In essence, RAG transforms generative AI from a highly knowledgeable but sometimes unreliable generalist into a highly knowledgeable, *reliable*, and *context-aware* specialist. It's the bridge that connects the impressive language generation capabilities of LLMs with the need for factual accuracy, currency, and domain specificity, making generative AI truly practical and powerful for countless real-world applications.\n",
      "\n",
      "It's a \"superpower\" because it gives LLMs the ability to \"look things up\" just like a human would, dramatically improving their utility and mitigating their inherent weaknesses.\n"
     ]
    }
   ],
   "source": [
    "print(response.text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68562ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "Ok so you are a one of the best and prodigy teacher in Ai and ML and right now you have to teach child who is bit slow in learning that how he can do the prompt engineering properly how he can write better and good prompts so you have to teach him and make sure you start from the scratch and he had no basics of the prompt enginnering.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2486341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template=ChatPromptTemplate.from_messages([(\"system\",prompt),(\"human\",\"{user_input}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ecd7c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "message=chat_template.format_messages(user_input=\"ok so tell me every basics and end to end thing about prompt enginnering and how do we write better prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "053e3eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there, my little friend! I'm so excited to teach you something super cool and powerful today. It's called **Prompt Engineering**, and it's like learning how to talk to a magical, super-smart robot (that's the AI!) so it understands you perfectly and helps you with anything you want.\n",
      "\n",
      "Don't worry if it sounds big and complicated. We'll go super slow, step-by-step, just like learning to ride a bike. We'll start from the very beginning, and I promise you'll be a prompt-writing wizard by the end!\n",
      "\n",
      "---\n",
      "\n",
      "### Part 1: What is a \"Prompt\" and \"Prompt Engineering\"? (The Magical Chat)\n",
      "\n",
      "Imagine you have a best friend who is super, super smart. Like, knows everything, can write stories, solve puzzles, and even draw! But this friend can't read your mind. You have to tell them *exactly* what you want.\n",
      "\n",
      "*   **What is a \"Prompt\"?**\n",
      "    *   A \"prompt\" is simply **what you type or say to the super-smart robot (AI)**. It's like asking your smart friend a question or giving them an instruction.\n",
      "    *   *Example:* If you want your friend to draw a cat, you wouldn't just say \"draw.\" You'd say, \"Please draw a fluffy orange cat with green eyes, sleeping on a comfy pillow.\" That whole sentence is your \"prompt\"!\n",
      "\n",
      "*   **What is \"Prompt Engineering\"?**\n",
      "    *   \"Prompt Engineering\" is the **art and science of writing really good prompts**. It's learning *how* to ask your super-smart robot friend in the best way possible so it understands you perfectly and gives you exactly what you hoped for (or even better!).\n",
      "    *   Think of it like being an architect for your words. You're building the perfect instruction for the AI.\n",
      "\n",
      "### Part 2: Why is Prompt Engineering Important? (Making Your Robot Friend Super Helpful!)\n",
      "\n",
      "Why can't we just say anything? Well, remember your super-smart friend?\n",
      "*   If you just say \"Draw a cat,\" your friend might draw a black cat, a skinny cat, a cat standing up, a cat running... there are so many types of cats!\n",
      "*   But if you say \"Draw a fluffy orange cat with green eyes, sleeping on a comfy pillow, in a cartoon style,\" then your friend knows *exactly* what you want and will draw something much closer to your idea.\n",
      "\n",
      "It's the same with AI!\n",
      "*   **Better Answers:** Good prompts help the AI give you much more accurate, useful, and creative answers.\n",
      "*   **Save Time:** You get what you want faster, without having to ask again and again.\n",
      "*   **Unlock AI's Full Power:** The AI is like a giant library and a super artist all in one. Good prompts help you unlock all its amazing abilities!\n",
      "\n",
      "---\n",
      "\n",
      "### Part 3: The Secret Ingredients of a Super Prompt (The Magic Recipe!)\n",
      "\n",
      "Okay, now for the fun part! How do we make a prompt super good? Think of it like baking a cake. You need the right ingredients in the right order.\n",
      "\n",
      "Here are the main \"ingredients\" for a great prompt:\n",
      "\n",
      "1.  **Role (Who are you?)**\n",
      "    *   Tell the AI *who* it should pretend to be. This helps it think from a specific perspective.\n",
      "    *   *Example:* \"You are a friendly storyteller.\" or \"You are a wise old wizard.\" or \"You are a helpful teacher.\"\n",
      "    *   *Why this helps:* A storyteller will use different words than a wizard, right?\n",
      "\n",
      "2.  **Task / Goal (What do you want to do?)**\n",
      "    *   This is the most important part! Clearly state *what you want the AI to do*.\n",
      "    *   *Example:* \"Write a short story.\" or \"Explain how a rainbow works.\" or \"Give me ideas for a birthday party.\"\n",
      "    *   *Why this helps:* It tells the AI its main job.\n",
      "\n",
      "3.  **Context / Information (What details does it need to know?)**\n",
      "    *   Give the AI all the important background information it needs to do its job well.\n",
      "    *   *Example:* (For a story) \"The story should be about a brave mouse and a kind elephant.\" (For a rainbow) \"Explain it in a way a 7-year-old can understand.\" (For party ideas) \"The party is for my 8-year-old sister who loves unicorns.\"\n",
      "    *   *Why this helps:* It narrows down the possibilities and guides the AI.\n",
      "\n",
      "4.  **Format / Output (How do you want the answer to look?)**\n",
      "    *   Tell the AI how you want its answer to be presented.\n",
      "    *   *Example:* \"Write it as a list.\" or \"Use simple sentences.\" or \"Write it as a poem.\" or \"Make it exactly 3 paragraphs long.\"\n",
      "    *   *Why this helps:* It makes the answer easy for you to use and read.\n",
      "\n",
      "5.  **Constraints / Rules (What *not* to do, or special rules)**\n",
      "    *   Sometimes, it's helpful to tell the AI what *not* to do, or any special rules it must follow.\n",
      "    *   *Example:* \"Do not use any big, complicated words.\" or \"Make sure it's happy and positive.\" or \"Do not mention scary things.\"\n",
      "    *   *Why this helps:* It prevents unwanted things and keeps the answer safe and suitable.\n",
      "\n",
      "6.  **Examples (Show, Don't Just Tell!) - *Optional but Super Powerful!***\n",
      "    *   If you have a very specific style or type of answer you want, show the AI an example! It's like saying, \"See this? I want something *like this*.\"\n",
      "    *   *Example:* If you want a specific style of riddle, give it one riddle and ask it to make another one in the same style.\n",
      "    *   *Why this helps:* It's the clearest way to communicate a style or pattern.\n",
      "\n",
      "---\n",
      "\n",
      "### Part 4: How to Write a Super Prompt - Step-by-Step (Let's Build One!)\n",
      "\n",
      "Let's try to build a prompt together, step-by-step, for a simple task: **Writing a short story.**\n",
      "\n",
      "**Step 1: Start Simple (Just the Task)**\n",
      "*   *Bad Prompt:* \"Write a story.\"\n",
      "*   *AI's answer:* Could be about anything! Might not be what you want.\n",
      "\n",
      "**Step 2: Add the Role (Who is telling the story?)**\n",
      "*   *Better Prompt:* \"You are a friendly storyteller. Write a story.\"\n",
      "*   *AI's answer:* The story will sound more engaging and warm.\n",
      "\n",
      "**Step 3: Add the Context (What should the story be about?)**\n",
      "*   *Even Better Prompt:* \"You are a friendly storyteller. Write a story about a brave little squirrel who loses his favorite acorn.\"\n",
      "*   *AI's answer:* Now it knows the main character and the problem!\n",
      "\n",
      "**Step 4: Add the Format (How long, how simple?)**\n",
      "*   *Getting Closer! Prompt:* \"You are a friendly storyteller. Write a short story (about 3 paragraphs) about a brave little squirrel who loses his favorite acorn. Make sure the language is simple, like for a 6-year-old.\"\n",
      "*   *AI's answer:* Now the story will be just the right length and easy to understand.\n",
      "\n",
      "**Step 5: Add Constraints (Any 'don'ts' or special rules?)**\n",
      "*   *Almost Perfect Prompt:* \"You are a friendly storyteller. Write a short story (about 3 paragraphs) about a brave little squirrel who loses his favorite acorn. Make sure the language is simple, like for a 6-year-old. **The story must have a happy ending where he finds the acorn.**\"\n",
      "*   *AI's answer:* Now we've guided it to a happy ending, which is important for a child's story!\n",
      "\n",
      "**Step 6: Review and Refine (Read it out loud!)**\n",
      "*   Read your prompt. Does it make sense? Is anything missing? Is it clear?\n",
      "*   \"You are a friendly storyteller. Write a short story (about 3 paragraphs) about a brave little squirrel who loses his favorite acorn. Make sure the language is simple, like for a 6-year-old. The story must have a happy ending where he finds the acorn.\"\n",
      "*   This looks like a great prompt!\n",
      "\n",
      "---\n",
      "\n",
      "### Part 5: Super Tips for Writing *Even Better* Prompts (Becoming a Prompt Wizard!)\n",
      "\n",
      "Here are some extra magic tricks to make your prompts even stronger:\n",
      "\n",
      "1.  **Be Super Clear and Specific:**\n",
      "    *   **Bad:** \"Tell me about dogs.\" (Too general!)\n",
      "    *   **Good:** \"Tell me 5 interesting facts about golden retriever puppies, specifically focusing on their playful nature.\" (Much clearer!)\n",
      "\n",
      "2.  **Use Action Words:**\n",
      "    *   Tell the AI *what to do*. Use words like: \"Create,\" \"Summarize,\" \"Explain,\" \"Generate,\" \"List,\" \"Compare,\" \"Brainstorm,\" \"Write,\" \"Analyze.\"\n",
      "\n",
      "3.  **Break Down Big Tasks:**\n",
      "    *   If you have a very big task, break it into smaller steps.\n",
      "    *   *Instead of:* \"Plan my whole birthday party.\"\n",
      "    *   *Try:* \"1. Brainstorm 5 theme ideas for an 8-year-old's birthday party. 2. For the 'Unicorn' theme, suggest 3 game ideas. 3. Suggest a simple menu for the 'Unicorn' party.\" (You can ask these one after another or combine them carefully).\n",
      "\n",
      "4.  **Experiment and Iterate (Try Different Things!):**\n",
      "    *   Don't be afraid to try a prompt, see the answer, and then change your prompt to get a better answer. It's like trying different keys to find the one that opens the treasure chest!\n",
      "    *   \"Hmm, that wasn't quite right. Let me add 'in a funny tone' to my prompt.\"\n",
      "\n",
      "5.  **Give Examples (Like showing a picture!):**\n",
      "    *   If you want a very specific style, give the AI an example.\n",
      "    *   *Prompt:* \"I want you to write a riddle about an apple. Here's an example of how I like riddles: 'I have cities, but no houses. I have mountains, but no trees. I have water, but no fish. What am I? (A map)' Now, write a riddle about an apple in a similar style.\"\n",
      "\n",
      "6.  **Ask for Step-by-Step Thinking (Show Your Work!):**\n",
      "    *   If you want the AI to solve a problem or explain something complex, ask it to think step-by-step.\n",
      "    *   *Prompt:* \"Explain how a seed grows into a flower. Please explain it step-by-step, starting from planting the seed.\"\n",
      "    *   *Why this helps:* It helps the AI organize its thoughts and gives you a clearer, more logical answer.\n",
      "\n",
      "7.  **Define Your Audience and Tone:**\n",
      "    *   Who is the answer for? How should it sound?\n",
      "    *   *Examples:* \"Explain this to a 5-year-old.\" or \"Write this in a formal tone.\" or \"Write this like a funny cartoon character.\"\n",
      "\n",
      "8.  **Use Negative Constraints (What *NOT* to do):**\n",
      "    *   Sometimes it's easier to say what you *don't* want.\n",
      "    *   *Prompt:* \"Write a short story about a cat. Do NOT include any dogs or birds.\"\n",
      "\n",
      "---\n",
      "\n",
      "### Part 6: Let's Practice Together!\n",
      "\n",
      "Imagine you want the AI to help you plan a fun afternoon.\n",
      "\n",
      "**Bad Prompt Example:**\n",
      "\"Fun things to do.\"\n",
      "*   *Why it's bad:* Too vague! The AI doesn't know *who* it's for, *when*, *where*, or *what kind* of fun.\n",
      "\n",
      "**Good Prompt Example:**\n",
      "\"You are a super fun activity planner for kids.\n",
      "**Task:** Suggest 3 exciting outdoor activities for me to do this afternoon.\n",
      "**Context:** I am 7 years old, I love animals and building things, and I have a big backyard with some old boxes.\n",
      "**Format:** List the activities with a short explanation for each.\n",
      "**Constraints:** Do not suggest anything that needs special equipment or a lot of money.\"\n",
      "\n",
      "See how much information we gave it? Now the AI knows exactly what kind of suggestions you want!\n",
      "\n",
      "---\n",
      "\n",
      "### Conclusion: You're Becoming a Prompt Wizard!\n",
      "\n",
      "Learning Prompt Engineering is like learning a secret language to talk to the most powerful tools in the world. It takes a little practice, but every time you try, you'll get better and better.\n",
      "\n",
      "Remember these key things:\n",
      "*   **Be Clear!**\n",
      "*   **Be Specific!**\n",
      "*   **Tell the AI its Role, Task, Context, and Format!**\n",
      "*   **Don't be afraid to experiment!**\n",
      "\n",
      "You've got this! Keep practicing, and soon you'll be making that super-smart AI do amazing things for you. What's the first thing you want to try prompting the AI to do? Let me know!"
     ]
    }
   ],
   "source": [
    "for chunks in chatmodel.stream(message):\n",
    "    print(chunks.content,end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f541f1",
   "metadata": {},
   "source": [
    "# Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f62abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples=[{\"input\":\"Hi\",\"output\":\"Salut\"},{\"input\":\"Good Morning\",\"output\":\"bonjour\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47728a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt=ChatPromptTemplate.from_messages([(\"human\",\"{input}\"),(\"ai\",\"{output}\")])\n",
    "few_shot_prompt=FewShotChatMessagePromptTemplate(example_prompt=example_prompt,examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abaab481",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a English-French translator.\"),few_shot_prompt,(\"human\",\"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289647b2",
   "metadata": {},
   "source": [
    "# Chains For Sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0685261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment allez-vous ? (formal)\n",
      "Comment vas-tu ? (informal)\n",
      "Comment ça va ? (common, versatile)\n"
     ]
    }
   ],
   "source": [
    "chain= final_prompt | chatmodel\n",
    "answer=chain.invoke({\"input\":\"How are you?\"})\n",
    "print(answer.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c69c3",
   "metadata": {},
   "source": [
    "# Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f659f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e291c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_prompt=PromptTemplate.from_template(\"Return a json object with a answer key that answers the following questions: {question}\")\n",
    "\n",
    "json_parser=SimpleJsonOutputParser()\n",
    "\n",
    "json_chain=json_prompt | chatmodel |  json_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b911432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=json_chain.invoke({\"question\":\"Which is the biggest country in the world and how much it is bigger than the smallest country and why china is not the biggest country\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0c65f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The biggest country in the world is Russia, with a total area of approximately 17,098,246 square kilometers. It is vastly larger than the smallest country, Vatican City (0.44 square kilometers), being approximately 38.8 million times bigger. China, with an area of about 9,596,961 square kilometers, is not the biggest country because Russia has a significantly larger land area.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3eec16",
   "metadata": {},
   "source": [
    "# It is customised outptut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81395f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3550: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel,Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6dc6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup:str =Field(description=\"question to set up a joke\")\n",
    "    punchline:str=Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "593f3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt=PromptTemplate(template=\"Answer the user query. \\n {format_instructions}\\n{query}\",input_variables=['query'],partial_variables={\"format_instructions\":parser.get_format_instructions()},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ba1db65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4456f50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91800\\AppData\\Local\\Temp\\ipykernel_15840\\3863542573.py:31: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chatmodel(filled_chat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly!\n",
      "\n",
      "Imagine you have a lot of data points, and each data point has many different features (like a person's age, height, weight, income, education level, etc.). This can be hard to visualize or process.\n",
      "\n",
      "**PCA (Principal Component Analysis) is like finding the most important \"angles\" or \"directions\" in your data that explain most of the variation, and then projecting your data onto these new, fewer directions.**\n",
      "\n",
      "Here's a simpler breakdown:\n",
      "\n",
      "1.  **Problem:** Too many features (high-dimensional data) can make analysis slow, difficult to visualize, and sometimes lead to less accurate models.\n",
      "2.  **PCA's Goal:** Reduce the number of features (dimensions) while keeping as much of the important information (variance) as possible.\n",
      "3.  **How it works (conceptually):**\n",
      "    *   It looks for new, independent \"axes\" (called Principal Components) in your data.\n",
      "    *   The **first principal component** is the direction along which your data varies the most.\n",
      "    *   The **second principal component** is another direction, perpendicular to the first, along which the remaining data varies the most, and so on.\n",
      "    *   You then select only the top few principal components that capture, say, 90% or 95% of the total variation in your original data.\n",
      "4.  **Result:** You end up with a smaller number of new features (the selected principal components) that are combinations of your original features, but they represent the most significant patterns in your data.\n",
      "\n",
      "**Think of it like this:** If you have a 3D object (data with 3 features) and you want to take a 2D picture (reduce to 2 features) that still shows most of its shape and details, PCA helps you find the best angle to take that picture.\n",
      "\n",
      "**Benefits:**\n",
      "*   **Reduces noise:** Less important variations are discarded.\n",
      "*   **Faster computation:** Less data for machine learning models to process.\n",
      "*   **Better visualization:** Easier to plot data when it has 2 or 3 dimensions instead of many.\n",
      "*   **Can improve model performance:** By removing redundant or noisy features.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "# 1️⃣ Define a reusable chat template\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a {profession} expert on {topic}. Always answer clearly and concisely.\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"Hello, {user_name}. I have a question for you.\"\n",
    "        ),\n",
    "        AIMessagePromptTemplate.from_template(\n",
    "            \"Sure, happy to help!\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{user_input}\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2️⃣ Fill in placeholders dynamically\n",
    "filled_chat = chat_template.format_messages(\n",
    "    profession=\"Data Scientist\",\n",
    "    topic=\"Machine Learning\",\n",
    "    user_name=\"Raghav\",\n",
    "    user_input=\"Can you explain PCA in simple terms?\"\n",
    ")\n",
    "\n",
    "# 3️⃣ Send this to your chat model\n",
    "# Example (assuming 'chat_model' is already defined):\n",
    "response = chatmodel(filled_chat)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b5519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
