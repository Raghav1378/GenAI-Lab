{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9da4653b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current key: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "import os\n",
    "print(\"Current key:\", os.environ.get(\"HUGGINGFACE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8574def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from langchain_core.runnables import Runnable\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class ChatModel2(Runnable):\n",
    "    \"\"\"\n",
    "    Runnable wrapper around the OpenAI-compatible client (used for HuggingFace router).\n",
    "    Works with LangChain Core pipelines (prompt | chatmodel2 | parser).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, api_key: str = None, base_url: str = None):\n",
    "        super().__init__()\n",
    "        # ✅ This line now looks for HUGGINGFACE_API_KEY, not HF_TOKEN\n",
    "        key = api_key or os.environ.get(\"HF_TOKEN\")\n",
    "        if not key:\n",
    "            raise ValueError(\"No Hugging Face API key found. Please set HUGGINGFACE_API_KEY in your .env file.\")\n",
    "        \n",
    "        self.client = OpenAI(\n",
    "            base_url = base_url or \"https://router.huggingface.co/v1\",\n",
    "            api_key = key,\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def invoke(self, inputs, config=None, **kwargs):\n",
    "        \"\"\"Accepts dict, string, or ChatPromptValue\"\"\"\n",
    "        try:\n",
    "            prompt_text = inputs.to_string()\n",
    "        except Exception:\n",
    "            if isinstance(inputs, dict):\n",
    "                prompt_text = inputs.get(\"prompt\") or inputs.get(\"topic\") or inputs.get(\"text\")\n",
    "            else:\n",
    "                prompt_text = str(inputs)\n",
    "\n",
    "        if not prompt_text or not prompt_text.strip():\n",
    "            raise ValueError(\"No prompt text provided to ChatModel2.\")\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model = self.model_name,\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "        )\n",
    "        return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c30ec75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sky appears blue to our eyes during daylight because of Rayleigh scattering: shorter (blue) wavelengths of sunlight are scattered more strongly by the gases in the atmosphere. At sunrise or sunset, the scattering favors longer (red/orange) wavelengths, making the sky appear those colors.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatmodel=ChatModel2(\"openai/gpt-oss-20b:groq\")\n",
    "\n",
    "chatmodel.invoke(\"what is the color of sky\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "303c843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "\n",
    "class ChatModel2_120B(Runnable):\n",
    "    \"\"\"\n",
    "    Runnable wrapper for the gpt-oss-120B model via the OpenAI-compatible Hugging Face router.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"openai/gpt-oss-120b:groq\",\n",
    "                 api_key: str = None, base_url: str = None):\n",
    "        super().__init__()\n",
    "        self.client = OpenAI(\n",
    "            base_url=base_url or \"https://router.huggingface.co/v1\",\n",
    "            api_key=api_key or os.environ.get(\"HF_TOKEN\"),\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def invoke(self, inputs=None, config=None, **kwargs):\n",
    "        # Gracefully handle missing input\n",
    "        if inputs is None:\n",
    "            raise ValueError(\"The 'inputs' argument cannot be None.\")\n",
    "\n",
    "        # Normalize input\n",
    "        if isinstance(inputs, dict):\n",
    "            prompt_text = inputs.get(\"prompt\") or inputs.get(\"topic\") or str(inputs)\n",
    "        else:\n",
    "            prompt_text = str(inputs)\n",
    "\n",
    "        prompt_text = prompt_text.strip()\n",
    "        if not prompt_text:\n",
    "            raise ValueError(\"No valid prompt text provided to ChatModel2_120B\")\n",
    "\n",
    "        # Generate completion\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "            **(config or {}),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Return message content safely (some clients return dicts, others objects)\n",
    "        choice = completion.choices[0]\n",
    "        message = getattr(choice, \"message\", None)\n",
    "        if isinstance(message, dict):\n",
    "            return message.get(\"content\", \"\")\n",
    "        elif hasattr(message, \"content\"):\n",
    "            return message.content\n",
    "        else:\n",
    "            return str(choice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6bf8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatModel2_120B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a7a3482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 1 equals 2.\n"
     ]
    }
   ],
   "source": [
    "print(chatmodel.invoke(\"what is 1+1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f3876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98dad949",
   "metadata": {},
   "source": [
    "# RunnablePassthrough \n",
    "\n",
    "##### 💠 It does not do anything to input data\n",
    "##### 💠 Lets see a eg so that we can get some understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44c1b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough,RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f28156f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnablePassthrough()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5f571983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Raghav'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chain.invoke(\"Raghav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6bc11a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def russian_lastname(name:str) -> str:\n",
    "    return f\"{name} Sama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59151007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Raghav Sama'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=RunnablePassthrough() | RunnableLambda(russian_lastname)\n",
    "\n",
    "chain.invoke(\"Raghav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33a0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48bdd7d7",
   "metadata": {},
   "source": [
    "# RunnableParallel\n",
    "\n",
    "### 🛹We will use the runnnableparallel for running tasks in parallel\n",
    "### 🛹This is probably the most important and most useful Runnabel from Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b84ea926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6dce4682",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=RunnableParallel(\n",
    "    {\n",
    "        \"operation_1\":RunnablePassthrough(),\n",
    "        \"opearation2\":RunnableLambda(russian_lastname)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e8db989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_1': 'Raghav', 'opearation2': 'Raghav Sama'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Raghav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d93a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8f03491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_template(\"tell me a curious fact about {player}\")\n",
    "\n",
    "output_parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6b265eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def russian_lastname_from_dictonary(person):\n",
    "    return person[\"name\"] + \"ovicH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "456fca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2=RunnableParallel(\n",
    "    {\n",
    "        \"player\":RunnablePassthrough(),\n",
    "        \"opearation2\":RunnableLambda(russian_lastname_from_dictonary),\n",
    "        \"operation3\":RunnablePassthrough(),\n",
    "\n",
    "    }\n",
    ") | prompt | chatmodel| output_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67c378b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here’s a neat connection between the two names:\\n\\n- **Abraham** (often rendered as *Abram* in older translations) is the biblical patriarch who journeys from Ur to the land that God promised to his descendants.  \\n- **Jordan** is the famous river that runs through the Middle East and is the waterway Abraham crossed when he entered the land of Canaan.\\n\\n**Curious fact:** In the Book of Genesis, Abraham and his family actually cross the Jordan River as part of the covenantal journey. This crossing is a pivotal moment—just as the name “Jordan” itself means “to descend,” Abraham’s name, meaning “father of many,” heralds the descent of a new, diverse peoples into the promised territory. It’s a little poetic that the patriarch who becomes the “father of many” literally steps into the river that bears a name meaning “to descend.”'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.invoke({\"name1\": \"Jordan\",\"name\":\"Abhram\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f133161",
   "metadata": {},
   "source": [
    "# Lets see a more use of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "be452021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough,RunnableParallel\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "32872345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "class QwenEmbeddingModel:\n",
    "    def __init__(self, api_key=None, provider=\"nebius\", model=\"Qwen/Qwen3-Embedding-8B\"):\n",
    "        self.api_key = api_key or os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Please set HUGGINGFACE_API_KEY environment variable or pass api_key explicitly.\")\n",
    "        \n",
    "        self.model = model\n",
    "        self.client = InferenceClient(provider=provider, api_key=self.api_key)\n",
    "\n",
    "    def embed_query(self, text: str):\n",
    "        \"\"\"Embed a single text and return a vector\"\"\"\n",
    "        result = self.client.feature_extraction(text, model=self.model)\n",
    "        return result[0]\n",
    "\n",
    "    def embed_documents(self, texts: list[str]):\n",
    "        \"\"\"Embed multiple texts and return a list of vectors\"\"\"\n",
    "        result = self.client.feature_extraction(texts, model=self.model)\n",
    "        return result\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        \"\"\"Allow the model to be called directly by FAISS or LangChain\"\"\"\n",
    "        return self.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d9370fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "class SentenceTransformerEmbeddingModel:\n",
    "    def __init__(self, api_key=None, provider=\"hf-inference\", model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.api_key = api_key or os.getenv(\"HF_TOKEN\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Please set HF_TOKEN environment variable or pass api_key explicitly.\")\n",
    "        \n",
    "        self.model = model\n",
    "        self.client = InferenceClient(provider=provider, api_key=self.api_key)\n",
    "\n",
    "    def embed_query(self, text: str):\n",
    "        result = self.client.feature_extraction(text, model=self.model)\n",
    "        # Ensure it's 2D for FAISS (1 sample, N dims)\n",
    "        return np.array(result).reshape(1, -1)\n",
    "\n",
    "    def embed_documents(self, texts: list[str]):\n",
    "        result = self.client.feature_extraction(texts, model=self.model)\n",
    "        return np.array(result)\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        return self.embed_query(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c9714cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=TextLoader(r'txt_files/ai_knowledge_base.txt',encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "text = \" \".join([doc.page_content for doc in docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8c0e79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model=QwenEmbeddingModel(api_key=os.getenv(\"HF_TOKEN\"))\n",
    "embedding_model2=SentenceTransformerEmbeddingModel()\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9290f6d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m text_split\u001b[38;5;241m=\u001b[39m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_text_splitters\\character.py:149\u001b[0m, in \u001b[0;36mRecursiveCharacterTextSplitter.split_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msplit_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Split the input text into smaller chunks based on predefined separators.\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m        List[str]: A list of text chunks obtained after splitting.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_separators\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_text_splitters\\character.py:109\u001b[0m, in \u001b[0;36mRecursiveCharacterTextSplitter._split_text\u001b[1;34m(self, text, separators)\u001b[0m\n\u001b[0;32m    107\u001b[0m     separator \u001b[38;5;241m=\u001b[39m _s\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_separator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    110\u001b[0m     separator \u001b[38;5;241m=\u001b[39m _s\n\u001b[0;32m    111\u001b[0m     new_separators \u001b[38;5;241m=\u001b[39m separators[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\re.py:201\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msearch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "text_split=splitter.split_text(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7d60ef6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "vector_store=FAISS.from_texts(texts=text_split,embedding=embedding_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "404e22e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver=vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5584944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\" Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e3a0b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6c3217b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain=(RunnableParallel({\"context\":retriver,\"question\":RunnablePassthrough()}) | prompt | chatmodel | StrOutputParser()\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0ce8a80f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m answer\u001b[38;5;241m=\u001b[39m\u001b[43mretrieval_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCompare convolutional neural networks (CNNs) and transformers in terms of architecture, data requirements, and computational cost. Why have transformers started replacing CNNs in computer vision tasks?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3243\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m   3242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3243\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3244\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3245\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config)\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:4000\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3995\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3996\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3997\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[0;32m   3998\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3999\u001b[0m         ]\n\u001b[1;32m-> 4000\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   4001\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   4002\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:4000\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3995\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3996\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3997\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[0;32m   3998\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3999\u001b[0m         ]\n\u001b[1;32m-> 4000\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   4001\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   4002\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3984\u001b[0m, in \u001b[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001b[1;34m(step, input_, config, key)\u001b[0m\n\u001b[0;32m   3978\u001b[0m child_config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m   3979\u001b[0m     config,\n\u001b[0;32m   3980\u001b[0m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[0;32m   3981\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   3982\u001b[0m )\n\u001b[0;32m   3983\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m-> 3984\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3986\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3988\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_core\\retrievers.py:263\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m kwargs_ \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 263\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28minput\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_\n\u001b[0;32m    265\u001b[0m     )\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    267\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_)\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_core\\vectorstores\\base.py:1067\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1065\u001b[0m kwargs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs \u001b[38;5;241m|\u001b[39m kwargs\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1067\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_)\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1069\u001b[0m     docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[0;32m   1071\u001b[0m             query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_\n\u001b[0;32m   1072\u001b[0m         )\n\u001b[0;32m   1073\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:643\u001b[0m, in \u001b[0;36mFAISS.similarity_search\u001b[1;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    625\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    630\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    631\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 643\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score(\n\u001b[0;32m    644\u001b[0m         query, k, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m, fetch_k\u001b[38;5;241m=\u001b[39mfetch_k, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    645\u001b[0m     )\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:516\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[1;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \n\u001b[0;32m    501\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;124;03m    L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    515\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_query(query)\n\u001b[1;32m--> 516\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score_by_vector(\n\u001b[0;32m    517\u001b[0m     embedding,\n\u001b[0;32m    518\u001b[0m     k,\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m,\n\u001b[0;32m    520\u001b[0m     fetch_k\u001b[38;5;241m=\u001b[39mfetch_k,\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    522\u001b[0m )\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:417\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score_by_vector\u001b[1;34m(self, embedding, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_L2:\n\u001b[0;32m    416\u001b[0m     faiss\u001b[38;5;241m.\u001b[39mnormalize_L2(vector)\n\u001b[1;32m--> 417\u001b[0m scores, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m docs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\faiss\\class_wrappers.py:347\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_search\u001b[1;34m(self, x, k, params, D, I, numeric_type)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreplacement_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, k, \u001b[38;5;241m*\u001b[39m, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, D\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, I\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, numeric_type \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mFloat32):\n\u001b[0;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the k nearest neighbors of the set of vectors x in the index.\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \n\u001b[0;32m    323\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m        When not enough results are found, the label is set to -1\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    348\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(x, _numeric_to_str(numeric_type))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "answer=retrieval_chain.invoke(\"Compare convolutional neural networks (CNNs) and transformers in terms of architecture, data requirements, and computational cost. Why have transformers started replacing CNNs in computer vision tasks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4e7bd6da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43manswer\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'answer' is not defined"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0437e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough,RunnableParallel\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a789810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "newchatmodel=ChatModel2(\"openai/gpt-oss-20b:groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "86626644",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=TextLoader(r'txt_files/ai_knowledge_base.txt',encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "text = \" \".join([doc.page_content for doc in docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e2650875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(r\"txt_files/ai_knowledge_base.txt\", encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "text = [doc.page_content for doc in docs]\n",
    "split_texts = splitter.split_text(\" \".join(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "97f698a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "vector_store2 = FAISS.from_texts(texts=split_texts, embedding=embedding_model2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "765e2ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver2=vector_store2.as_retriever()\n",
    "template=\"\"\" Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following Language : {language}\n",
    "\"\"\"\n",
    "prompt=ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9ce15cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain3 = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\")\n",
    "        | retriver2\n",
    "        | (lambda docs: \"\\n\\n\".join([d.page_content for d in docs])),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | chatmodel\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "44732133",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mchain3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCompare convolutional neural networks (CNNs) and transformers in terms of architecture, data requirements, and computational cost. Why have transformers started replacing CNNs in computer vision tasks? \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3243\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m   3242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3243\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3244\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3245\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config)\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:4000\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3995\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3996\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3997\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[0;32m   3998\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3999\u001b[0m         ]\n\u001b[1;32m-> 4000\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   4001\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   4002\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:4000\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3995\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3996\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3997\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[0;32m   3998\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3999\u001b[0m         ]\n\u001b[1;32m-> 4000\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   4001\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   4002\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3984\u001b[0m, in \u001b[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001b[1;34m(step, input_, config, key)\u001b[0m\n\u001b[0;32m   3978\u001b[0m child_config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m   3979\u001b[0m     config,\n\u001b[0;32m   3980\u001b[0m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[0;32m   3981\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   3982\u001b[0m )\n\u001b[0;32m   3983\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m-> 3984\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3986\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3988\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3245\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3243\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3244\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3245\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3246\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3247\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_core\\retrievers.py:263\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m kwargs_ \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 263\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28minput\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_\n\u001b[0;32m    265\u001b[0m     )\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    267\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_)\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_core\\vectorstores\\base.py:1067\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1065\u001b[0m kwargs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs \u001b[38;5;241m|\u001b[39m kwargs\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1067\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_)\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1069\u001b[0m     docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[0;32m   1071\u001b[0m             query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_\n\u001b[0;32m   1072\u001b[0m         )\n\u001b[0;32m   1073\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:643\u001b[0m, in \u001b[0;36mFAISS.similarity_search\u001b[1;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    625\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    630\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    631\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 643\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score(\n\u001b[0;32m    644\u001b[0m         query, k, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m, fetch_k\u001b[38;5;241m=\u001b[39mfetch_k, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    645\u001b[0m     )\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:516\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[1;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \n\u001b[0;32m    501\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;124;03m    L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    515\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_query(query)\n\u001b[1;32m--> 516\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score_by_vector(\n\u001b[0;32m    517\u001b[0m     embedding,\n\u001b[0;32m    518\u001b[0m     k,\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m,\n\u001b[0;32m    520\u001b[0m     fetch_k\u001b[38;5;241m=\u001b[39mfetch_k,\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    522\u001b[0m )\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:417\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score_by_vector\u001b[1;34m(self, embedding, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_L2:\n\u001b[0;32m    416\u001b[0m     faiss\u001b[38;5;241m.\u001b[39mnormalize_L2(vector)\n\u001b[1;32m--> 417\u001b[0m scores, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m docs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\91800\\Desktop\\GENAI UDEMY\\venv\\lib\\site-packages\\faiss\\class_wrappers.py:347\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_search\u001b[1;34m(self, x, k, params, D, I, numeric_type)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreplacement_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, k, \u001b[38;5;241m*\u001b[39m, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, D\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, I\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, numeric_type \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mFloat32):\n\u001b[0;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the k nearest neighbors of the set of vectors x in the index.\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \n\u001b[0;32m    323\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m        When not enough results are found, the label is set to -1\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    348\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(x, _numeric_to_str(numeric_type))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "chain3.invoke({\"question\": \"Compare convolutional neural networks (CNNs) and transformers in terms of architecture, data requirements, and computational cost. Why have transformers started replacing CNNs in computer vision tasks? \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f8929e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d369a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
