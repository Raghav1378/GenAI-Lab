{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01433c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain._api import LangChainDeprecationWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=LangChainDeprecationWarning)\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1debadf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0bb65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "class LlamaRouter:\n",
    "    \"\"\"\n",
    "    A simple, reusable wrapper for using Meta Llama (or any Hugging Face model)\n",
    "    via the OpenAI-compatible Hugging Face Inference endpoint.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"meta-llama/Llama-3.1-8B-Instruct:novita\",\n",
    "        token_env: str = \"HF_TOKEN\",\n",
    "        base_url: str = \"https://router.huggingface.co/v1\",\n",
    "    ):\n",
    "        load_dotenv()\n",
    "        api_key = os.getenv(token_env)\n",
    "        if not api_key:\n",
    "            raise ValueError(f\"Missing token: environment variable '{token_env}' not found.\")\n",
    "        self.client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "        self.model = model\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def invoke(self, prompt: str, temperature: float = 0.7, max_tokens: int = 256) -> str:\n",
    "        \"\"\"Send a prompt string to the model and return the reply.\"\"\"\n",
    "        if not isinstance(prompt, str):\n",
    "            return \"[Error] Prompt must be a string.\"\n",
    "        try:\n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            return completion.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            return f\"[Error] {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a28aaa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "class GemmaRouter:\n",
    "    \"\"\"\n",
    "    A simple wrapper for using Google's Gemma-2-2B-IT model\n",
    "    via Hugging Face's OpenAI-compatible inference API.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"google/gemma-2-2b-it:nebius\",\n",
    "        token_env: str = \"HF_THIRD_TOKEN\",\n",
    "        base_url: str = \"https://router.huggingface.co/v1\",\n",
    "    ):\n",
    "        load_dotenv()\n",
    "        api_key = os.getenv(token_env)\n",
    "        if not api_key:\n",
    "            raise ValueError(\n",
    "                f\"Missing token: environment variable '{token_env}' not found.\"\n",
    "            )\n",
    "\n",
    "        self.client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "        self.model = model\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def invoke(self, prompt: str, temperature: float = 0.7, max_tokens: int = 256) -> str:\n",
    "        \"\"\"Send a prompt string to the Gemma model and return the reply.\"\"\"\n",
    "        if not isinstance(prompt, str):\n",
    "            return \"[Error] Prompt must be a string.\"\n",
    "\n",
    "        try:\n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            return completion.choices[0].message.content.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"[Error] {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b3e65a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-2.5-flash')\n",
    "model2 = LlamaRouter(token_env=\"HF_TOKEN\")\n",
    "model3 = LlamaRouter(token_env=\"HF_SECOND_TOKEN\")\n",
    "model4 = LlamaRouter(token_env=\"HF_THIRD_TOKEN\")\n",
    "gemma=GemmaRouter()\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "gemma_runnable = RunnableLambda(lambda x, **kwargs: gemma.invoke(x[\"content\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12eead6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder,HumanMessagePromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory,FileChatMessageHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b17e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=ConversationBufferMemory(chat_memory=FileChatMessageHistory(\"message.json\"),memory_key=\"message\",return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8fa4e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    messages=[\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f728db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(\n",
    "    llm=gemma_runnable,\n",
    "    prompt=prompt,\n",
    "    memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "430ce257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Hello how are you?', 'messages': [HumanMessage(content='Hello how are you?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! I'm doing well, thank you for asking.\\n\\nHow are you doing today? Is there anything I can help you with?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello how are you?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello again! I'm still doing well, thank you for asking.\\n\\nHow are you doing this time? Is there something specific you'd like to discuss or need help with?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello how are you?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! I'm still doing well, thank you for asking again.\\n\\nAs before, I'm ready to help with whatever you need. Is everything alright on your end, or did you have something specific you wanted to discuss today?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello how are you?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! I'm still doing well, thank you for asking for the fourth time.\\n\\nIt seems like you're repeatedly asking the same question. Is everything okay? Did you mean to ask something else, or is there a specific topic you'd like to discuss or need assistance with? I'm here and ready to help!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello how are you?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! I'm still doing well, thank you for asking again.\\n\\nYou've asked this question a few times now. Is there something specific you'd like to talk about, or a different question you meant to ask? I'm here and ready to assist you with anything you need!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello how are you?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! I'm still doing well, thank you for asking yet again.\\n\\nIt seems you're repeatedly asking the same question. Is everything alright, or is there a particular reason you're asking this multiple times?\\n\\nI'm here and ready to help you with any questions you might have or anything you'd like to discuss!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='My name is Panda', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Panda! It's nice to meet you.\\n\\nThank you for sharing your name. Is there something you'd like to talk about or something I can help you with today, Panda?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello how are you?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Panda! I'm still doing well, thank you for asking.\\n\\nIt's good to hear from you again. Is there something specific you'd like to talk about or a question I can help you with today, Panda?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='My name is Panda', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Panda! Thank you for letting me know again. It's nice to connect with you.\\n\\nIs there anything specific you'd like to discuss or something I can help you with today, Panda? I'm here to assist you.\", additional_kwargs={}, response_metadata={})], 'text': \"Hello Panda! I'm still doing well, thank you for asking.\\n\\nIt's good to hear from you. Is there something specific you'd like to talk about or a question I can help you with today, Panda?\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "gemma_runnable = RunnableLambda(lambda x: gemma.invoke(x[\"content\"]))\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    chat_memory=FileChatMessageHistory(\"message.json\"),\n",
    "    memory_key=\"messages\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    messages=[\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ],\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=model,\n",
    "    prompt=prompt,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"content\": \"Hello how are you?\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e6b09b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'what is',\n",
       " 'messages': [HumanMessage(content='Hello how are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello! I'm doing well, thank you for asking.\\n\\nHow are you doing today? Is there anything I can help you with?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Hello how are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello again! I'm still doing well, thank you for asking.\\n\\nHow are you doing this time? Is there something specific you'd like to discuss or need help with?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Hello how are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello! I'm still doing well, thank you for asking again.\\n\\nAs before, I'm ready to help with whatever you need. Is everything alright on your end, or did you have something specific you wanted to discuss today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Hello how are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello! I'm still doing well, thank you for asking for the fourth time.\\n\\nIt seems like you're repeatedly asking the same question. Is everything okay? Did you mean to ask something else, or is there a specific topic you'd like to discuss or need assistance with? I'm here and ready to help!\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Hello how are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello! I'm still doing well, thank you for asking again.\\n\\nYou've asked this question a few times now. Is there something specific you'd like to talk about, or a different question you meant to ask? I'm here and ready to assist you with anything you need!\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Hello how are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello! I'm still doing well, thank you for asking yet again.\\n\\nIt seems you're repeatedly asking the same question. Is everything alright, or is there a particular reason you're asking this multiple times?\\n\\nI'm here and ready to help you with any questions you might have or anything you'd like to discuss!\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is Panda', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello Panda! It's nice to meet you.\\n\\nThank you for sharing your name. Is there something you'd like to talk about or something I can help you with today, Panda?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Hello how are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello Panda! I'm still doing well, thank you for asking.\\n\\nIt's good to hear from you again. Is there something specific you'd like to talk about or a question I can help you with today, Panda?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is Panda', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello Panda! Thank you for letting me know again. It's nice to connect with you.\\n\\nIs there anything specific you'd like to discuss or something I can help you with today, Panda? I'm here to assist you.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Hello how are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello Panda! I'm still doing well, thank you for asking.\\n\\nIt's good to hear from you. Is there something specific you'd like to talk about or a question I can help you with today, Panda?\", additional_kwargs={}, response_metadata={})],\n",
       " 'text': '\"What is\" what, Panda?\\n\\nPlease complete your question, and I\\'ll do my best to answer it!'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"what is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543f751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee3920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b8fd7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f6818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee95f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b35d7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bappy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
