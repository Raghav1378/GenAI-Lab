{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc2b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai  import ChatGoogleGenerativeAI\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09ea44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "load_dotenv()\n",
    "class GemmaRouter:\n",
    "    \"\"\"\n",
    "    A simple wrapper for using Google's Gemma-2-2B-IT model\n",
    "    via Hugging Face's OpenAI-compatible inference API.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"google/gemma-2-2b-it:nebius\",\n",
    "        token_env: str = \"HF_THIRD_TOKEN\",\n",
    "        base_url: str = \"https://router.huggingface.co/v1\",\n",
    "    ):\n",
    "        load_dotenv()\n",
    "        api_key = os.getenv(token_env)\n",
    "        if not api_key:\n",
    "            raise ValueError(\n",
    "                f\"Missing token: environment variable '{token_env}' not found.\"\n",
    "            )\n",
    "\n",
    "        self.client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "    def invoke(self, prompt: str, temperature: float = 0.7, max_tokens: int = 256):\n",
    "        \"\"\"Non-streaming response.\"\"\"\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return completion.choices[0].message.content.strip()\n",
    "\n",
    "    def stream_invoke(self, prompt: str, temperature: float = 0.7, max_tokens: int = 256):\n",
    "        \"\"\"Streaming response.\"\"\"\n",
    "        stream = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            stream=True,\n",
    "        )\n",
    "        return stream\n",
    "model2=GemmaRouter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec548575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "messageToChatbot=[HumanMessage(content=\"My favourite color is blue\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c0b486d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's awesome! Blue is a beautiful and calming color.  \\n\\nDo you have a favorite shade of blue? ðŸ˜Š ðŸ’™\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.invoke(messageToChatbot[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0873b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb8003",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbotMemory={}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in chatbotMemory:\n",
    "        chatbotMemory[session_id]=ChatMessageHistory()\n",
    "    return chatbotMemory[session_id]\n",
    "\n",
    "chatbot_with_message_history=RunnableWithMessageHistory(model2,get_session_history=get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefb293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bappy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
